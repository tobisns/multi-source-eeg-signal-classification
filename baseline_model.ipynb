{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a6d0124",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from mne.decoding import CSP\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "import moabb\n",
    "from moabb.datasets import BNCI2014_001, Zhou2016, Schirrmeister2017, Weibo2014\n",
    "from moabb.evaluations import WithinSessionEvaluation\n",
    "from moabb.paradigms import LeftRightImagery\n",
    "\n",
    "moabb.set_log_level(\"info\")\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# moabb.set_download_dir(\"D:\\TA\\database\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705ffa9d",
   "metadata": {},
   "source": [
    "DATA LOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab398a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class MultisourceDataset(Dataset):\n",
    "    def __init__(self, X, YD, channel_mask):\n",
    "        self.X = X\n",
    "        self.YD = YD\n",
    "        self.channel_mask = channel_mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx]\n",
    "        yd = self.YD[idx]\n",
    "        masks = self.channel_mask[idx]\n",
    "        return x, yd, masks  # Return data + index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a3eca9",
   "metadata": {},
   "source": [
    "eeg_data_200 = multi-source dataset\n",
    "eeg_data_200_cross = multi-source dataset for cross subject classification\n",
    "eeg_data_bnci = BNCI2014_001 subset of multi-source dataset\n",
    "eeg_data_zhou = Zhou2016 subset of multi-source dataset\n",
    "eeg_data_weibo = Weibo2014 subset of multi-source dataset\n",
    "\n",
    "X = training rows/trials\n",
    "YD = training labels\n",
    "mask = training channel masks\n",
    "\n",
    "X_val = validatiom rows/trials\n",
    "YD_val = validation labels\n",
    "mask_val = validation channel masks\n",
    "\n",
    "X_test = test rows/trials\n",
    "YD_test = test labels\n",
    "mask_test = test channel masks\n",
    "\n",
    "channel_xy = list of channels positions\n",
    "\n",
    "Labels = [class, dataset sources id, subject id (of each dataset), session id (of each subject), domain id]\n",
    "domain id is unique\n",
    "combination (dataset sources id, subject id) is unique\n",
    "combination (dataset sources id, subject id, session id) is unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "feca0513",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "data = torch.load('./Dataset/eeg_data_200.pt', map_location=device)\n",
    "\n",
    "X_tensor = data['X']\n",
    "YD_tensor = data['YD']\n",
    "padding_masks_tensor = data['mask']\n",
    "channel_xy_tensor = data['channel_xy']\n",
    "\n",
    "X_tensor_half = data['X_val']\n",
    "YD_tensor_half = data['YD_val']\n",
    "padding_masks_tensor_half = data['mask_val']\n",
    "\n",
    "X_tensor_test = data['X_test']\n",
    "YD_tensor_test = data['YD_test']\n",
    "padding_masks_tensor_test = data['mask_test']\n",
    "\n",
    "dataset = MultisourceDataset(X_tensor, YD_tensor, padding_masks_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=10, shuffle=True)\n",
    "dataset_test = MultisourceDataset(X_tensor_half, YD_tensor_half, padding_masks_tensor_half)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=10, shuffle=True)\n",
    "dataset_test0 = MultisourceDataset(X_tensor_test, YD_tensor_test, padding_masks_tensor_test)\n",
    "dataloader_test0 = DataLoader(dataset_test0, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9720b8",
   "metadata": {},
   "source": [
    "EEGNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6a9e7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class EEGNet0(nn.Module):\n",
    "    def __init__(self, nb_classes, Chans=64, Samples=128, dropoutRate=0.5,\n",
    "                 kernLength=64, F1=8, D=2, F2=16, norm_rate=0.25):\n",
    "        super(EEGNet0, self).__init__()\n",
    "\n",
    "        # First temporal convolution\n",
    "        self.conv1 = nn.Conv2d(1, F1, (1, kernLength), padding=(0, kernLength // 2), bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(F1)\n",
    "\n",
    "        # Depthwise convolution (spatial filter)\n",
    "        self.depthwiseConv = nn.Conv2d(F1, F1 * D, (Chans, 1), groups=F1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(F1 * D)\n",
    "\n",
    "        # Pool and dropout after depthwise\n",
    "        self.pool1 = nn.AvgPool2d((1, 4))\n",
    "        self.drop1 = nn.Dropout(dropoutRate)\n",
    "\n",
    "        # Separable convolution (depthwise + pointwise)\n",
    "        self.sep_depth = nn.Conv2d(F1 * D, F1 * D, (1, 16), padding=(0, 8), groups=F1 * D, bias=False)\n",
    "        self.sep_point = nn.Conv2d(F1 * D, F2, (1, 1), bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(F2)\n",
    "\n",
    "        # Pool and dropout after separable conv\n",
    "        self.pool2 = nn.AvgPool2d((1, 8))\n",
    "        self.drop2 = nn.Dropout(dropoutRate)\n",
    "\n",
    "        # Final classifier\n",
    "        self.classifier = nn.Linear(F2 * ((Samples // 32)), nb_classes)  # Adapt to pooling sizes\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, 1, Chans, Samples)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "\n",
    "        x = self.depthwiseConv(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.elu(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.drop1(x)\n",
    "\n",
    "        x = self.sep_depth(x)\n",
    "        x = self.sep_point(x)\n",
    "        x = self.bn3(x)\n",
    "        x = F.elu(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.drop2(x)\n",
    "\n",
    "        x = x.flatten(start_dim=1)\n",
    "        logits = self.classifier(x)\n",
    "        return logits, x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20580a7c",
   "metadata": {},
   "source": [
    "DATA AUGMENTATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19547fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "\n",
    "def add_gaussian_noise_classwise(X, YD, mask, n_aug=1, N=5):\n",
    "    # Group trial indices by class\n",
    "    class_to_indices = {}\n",
    "    for i in range(X.shape[0]):\n",
    "        label = int(YD[i][0].item())\n",
    "        class_to_indices.setdefault(label, []).append(i)\n",
    "\n",
    "    aug_X, aug_YD, aug_mask = [], [], []\n",
    "\n",
    "    for label, indices in class_to_indices.items():\n",
    "        # Shuffle indices to ensure randomness\n",
    "        random.shuffle(indices)\n",
    "\n",
    "        # Partition into groups of N (e.g., 5)\n",
    "        for i in range(0, len(indices) - N + 1, N):  # ensures full groups\n",
    "            group = indices[i:i+N]  # size N group\n",
    "            group_trials = torch.stack([X[j] for j in group])  # [N, C, T]\n",
    "            group_mean = group_trials.mean().item()\n",
    "\n",
    "            for j in group:\n",
    "                x_i = X[j]\n",
    "                for _ in range(n_aug):\n",
    "                    noise = torch.randn_like(x_i) * group_mean\n",
    "                    x_aug = x_i + noise\n",
    "                    aug_X.append(x_aug.unsqueeze(0))\n",
    "                    aug_YD.append(YD[j].unsqueeze(0))\n",
    "                    aug_mask.append(mask[j].unsqueeze(0))\n",
    "\n",
    "    return (\n",
    "        torch.cat(aug_X, dim=0),\n",
    "        torch.cat(aug_YD, dim=0),\n",
    "        torch.cat(aug_mask, dim=0),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bcfab64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recombination_in_time(X, YD, mask, n_aug=1, n_chunks=5, output_length=None):\n",
    "    \"\"\"\n",
    "    Recombine time segments from different same-class samples.\n",
    "    Pads or trims to match `output_length`.\n",
    "    \"\"\"\n",
    "    augmented_X, augmented_YD, augmented_mask = [], [], []\n",
    "    classes = torch.unique(YD[:, 0])\n",
    "\n",
    "    if output_length is None:\n",
    "        output_length = X.shape[2]  # default to original trial length\n",
    "\n",
    "    for cls in classes:\n",
    "        idx_cls = (YD[:, 0] == cls).nonzero(as_tuple=True)[0]\n",
    "        if len(idx_cls) < n_chunks:\n",
    "            continue\n",
    "\n",
    "        for _ in range(n_aug * len(idx_cls)):\n",
    "            chosen = idx_cls[torch.randperm(len(idx_cls))[:n_chunks]]\n",
    "            segments = []\n",
    "            total_length = 0\n",
    "            chunk_target = output_length // n_chunks\n",
    "\n",
    "            for ch in chosen:\n",
    "                trial = X[ch]\n",
    "                trial_len = trial.shape[1]\n",
    "                if trial_len < chunk_target:\n",
    "                    continue  # skip if not enough length\n",
    "\n",
    "                start = torch.randint(0, trial_len - chunk_target + 1, (1,)).item()\n",
    "                segment = trial[:, start:start + chunk_target]\n",
    "                segments.append(segment)\n",
    "                total_length += segment.shape[1]\n",
    "\n",
    "            if len(segments) != n_chunks:\n",
    "                continue  # skip incomplete recombinations\n",
    "\n",
    "            new_trial = torch.cat(segments, dim=1)\n",
    "\n",
    "            # Ensure exact output length (pad or trim)\n",
    "            if new_trial.shape[1] < output_length:\n",
    "                pad_len = output_length - new_trial.shape[1]\n",
    "                pad = torch.zeros((new_trial.shape[0], pad_len), dtype=new_trial.dtype, device=new_trial.device)\n",
    "                new_trial = torch.cat([new_trial, pad], dim=1)\n",
    "            elif new_trial.shape[1] > output_length:\n",
    "                new_trial = new_trial[:, :output_length]\n",
    "\n",
    "            new_mask = mask[chosen[0]].clone()\n",
    "\n",
    "\n",
    "            augmented_X.append(new_trial.unsqueeze(0))\n",
    "            augmented_YD.append(YD[chosen[0]].unsqueeze(0))\n",
    "            augmented_mask.append(new_mask.unsqueeze(0))\n",
    "\n",
    "    if augmented_X:\n",
    "        return (\n",
    "            torch.cat(augmented_X, dim=0),\n",
    "            torch.cat(augmented_YD, dim=0),\n",
    "            torch.cat(augmented_mask, dim=0)\n",
    "        )\n",
    "    else:\n",
    "        return None, None, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b86389",
   "metadata": {},
   "source": [
    "TRAIN LOOP NORMAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abe58a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "for indSubj in range(9):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    data = torch.load('./Datasets/eeg_data_bnci.pt', map_location=device)\n",
    "\n",
    "    X_tensor = data['X']\n",
    "    YD_tensor = data['YD']\n",
    "    padding_masks_tensor = data['mask']\n",
    "    channel_xy_tensor = data['channel_xy']\n",
    "\n",
    "    X_tensor_half = data['X_val']\n",
    "    YD_tensor_half = data['YD_val']\n",
    "    padding_masks_tensor_half = data['mask_val']\n",
    "\n",
    "    X_tensor_test = data['X_test']\n",
    "    YD_tensor_test = data['YD_test']\n",
    "    padding_masks_tensor_test = data['mask_test']\n",
    "\n",
    "    idx = YD_tensor[:, 4] == indSubj + 4\n",
    "    idx_half = YD_tensor_half[:, 4] == indSubj + 4\n",
    "    idx_test = YD_tensor_test[:, 4] == indSubj + 4\n",
    "\n",
    "    X_tensor = X_tensor[idx]\n",
    "    YD_tensor = YD_tensor[idx]\n",
    "    padding_masks_tensor = padding_masks_tensor[idx]\n",
    "\n",
    "    X_tensor_half = X_tensor_half[idx_half]\n",
    "    YD_tensor_half = YD_tensor_half[idx_half]\n",
    "    padding_masks_tensor_half = padding_masks_tensor_half[idx_half]\n",
    "\n",
    "    X_tensor_test = X_tensor_test[idx_test]\n",
    "    YD_tensor_test = YD_tensor_test[idx_test]\n",
    "    padding_masks_tensor_test = padding_masks_tensor_test[idx_test]\n",
    "\n",
    "    dataset = MultisourceDataset(X_tensor, YD_tensor, padding_masks_tensor)\n",
    "    dataloader = DataLoader(dataset, batch_size=10, shuffle=True)\n",
    "    dataset_test = MultisourceDataset(X_tensor_half, YD_tensor_half, padding_masks_tensor_half)\n",
    "    dataloader_test = DataLoader(dataset_test, batch_size=10, shuffle=True)\n",
    "    dataset_test0 = MultisourceDataset(X_tensor_test, YD_tensor_test, padding_masks_tensor_test)\n",
    "    dataloader_test0 = DataLoader(dataset_test0, batch_size=10, shuffle=True)\n",
    "\n",
    "    seed = 42\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    out_channels = 22\n",
    "    num_classes = 3\n",
    "    learning_rate = 0.0005\n",
    "    num_epochs = 500\n",
    "\n",
    "    model = EEGNet0(nb_classes=num_classes, Chans=out_channels, Samples=841).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "\n",
    "    train_loader = DataLoader(dataloader.dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(dataloader_test.dataset, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(dataloader_test0.dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    best_val_acc = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        correct, total, running_loss = 0, 0, 0\n",
    "        for batch in train_loader:\n",
    "            x_batch, yd_batch, mask_batch = batch\n",
    "            X_valid = x_batch[mask_batch].reshape(x_batch.shape[0], -1, x_batch.shape[-1])\n",
    "            X_valid = X_valid.unsqueeze(1).to(device)\n",
    "            yd_batch = yd_batch.to(device)\n",
    "\n",
    "            output, _ = model(X_valid)\n",
    "            loss = criterion(output, yd_batch[:, 0])\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            predicted_labels = torch.argmax(output, dim=1)\n",
    "            correct += (predicted_labels == yd_batch[:, 0]).sum().item()\n",
    "            total += yd_batch.size(0)\n",
    "\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_acc = correct / total * 100\n",
    "\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                x_batch, yd_batch, mask_batch = batch\n",
    "                X_valid = x_batch[mask_batch].reshape(x_batch.shape[0], -1, x_batch.shape[-1])\n",
    "                X_valid = X_valid.unsqueeze(1).to(device)\n",
    "                yd_batch = yd_batch.to(device)\n",
    "\n",
    "                output, _ = model(X_valid)\n",
    "                loss = criterion(output, yd_batch[:, 0])\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                predicted_labels = torch.argmax(output, dim=1)\n",
    "                val_correct += (predicted_labels == yd_batch[:, 0]).sum().item()\n",
    "                val_total += yd_batch.size(0)\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc = val_correct / val_total * 100\n",
    "\n",
    "        test_loss, test_correct, test_total = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                x_batch, yd_batch, mask_batch = batch\n",
    "                X_valid = x_batch[mask_batch].reshape(x_batch.shape[0], -1, x_batch.shape[-1])\n",
    "                X_valid = X_valid.unsqueeze(1).to(device)\n",
    "                yd_batch = yd_batch.to(device)\n",
    "\n",
    "                output, _ = model(X_valid)\n",
    "                loss = criterion(output, yd_batch[:, 0])\n",
    "                test_loss += loss.item()\n",
    "\n",
    "                predicted_labels = torch.argmax(output, dim=1)\n",
    "                test_correct += (predicted_labels == yd_batch[:, 0]).sum().item()\n",
    "                test_total += yd_batch.size(0)\n",
    "\n",
    "        test_loss /= len(test_loader)\n",
    "        test_acc = test_correct / test_total * 100\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            save_path = f\"./EEGNet0-BNCI-s{indSubj}-TEST.pth\"\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, save_path)\n",
    "\n",
    "            print(\"✅ Model saved.\")\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% | \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}% | \"\n",
    "              f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59119c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "for indSubj in range(10):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    data = torch.load('./Dataset/eeg_data_weibo.pt', map_location=device)\n",
    "\n",
    "    X_tensor = data['X']\n",
    "    YD_tensor = data['YD']\n",
    "    padding_masks_tensor = data['mask']\n",
    "    channel_xy_tensor = data['channel_xy']\n",
    "\n",
    "    X_tensor_half = data['X_val']\n",
    "    YD_tensor_half = data['YD_val']\n",
    "    padding_masks_tensor_half = data['mask_val']\n",
    "\n",
    "    X_tensor_test = data['X_test']\n",
    "    YD_tensor_test = data['YD_test']\n",
    "    padding_masks_tensor_test = data['mask_test']\n",
    "\n",
    "    idx = YD_tensor[:, 4] == indSubj + 13\n",
    "    idx_half = YD_tensor_half[:, 4] == indSubj + 13\n",
    "    idx_test = YD_tensor_test[:, 4] == indSubj + 13\n",
    "\n",
    "    X_tensor = X_tensor[idx]\n",
    "    YD_tensor = YD_tensor[idx]\n",
    "    padding_masks_tensor = padding_masks_tensor[idx]\n",
    "\n",
    "    X_tensor_half = X_tensor_half[idx_half]\n",
    "    YD_tensor_half = YD_tensor_half[idx_half]\n",
    "    padding_masks_tensor_half = padding_masks_tensor_half[idx_half]\n",
    "\n",
    "    X_tensor_test = X_tensor_test[idx_test]\n",
    "    YD_tensor_test = YD_tensor_test[idx_test]\n",
    "    padding_masks_tensor_test = padding_masks_tensor_test[idx_test]\n",
    "\n",
    "    dataset = MultisourceDataset(X_tensor, YD_tensor, padding_masks_tensor)\n",
    "    dataloader = DataLoader(dataset, batch_size=10, shuffle=True)\n",
    "    dataset_test = MultisourceDataset(X_tensor_half, YD_tensor_half, padding_masks_tensor_half)\n",
    "    dataloader_test = DataLoader(dataset_test, batch_size=10, shuffle=True)\n",
    "    dataset_test0 = MultisourceDataset(X_tensor_test, YD_tensor_test, padding_masks_tensor_test)\n",
    "    dataloader_test0 = DataLoader(dataset_test0, batch_size=10, shuffle=True)\n",
    "\n",
    "    seed = 42\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    out_channels = 60\n",
    "    num_classes = 3\n",
    "    learning_rate = 0.0005\n",
    "    num_epochs = 500\n",
    "\n",
    "    model = EEGNet0(nb_classes=num_classes, Chans=out_channels, Samples=841).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "\n",
    "    train_loader = DataLoader(dataloader.dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(dataloader_test.dataset, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(dataloader_test0.dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    best_val_acc = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        correct, total, running_loss = 0, 0, 0\n",
    "        for batch in train_loader:\n",
    "            x_batch, yd_batch, mask_batch = batch\n",
    "            X_valid = x_batch[mask_batch].reshape(x_batch.shape[0], -1, x_batch.shape[-1])\n",
    "            X_valid = X_valid.unsqueeze(1).to(device)\n",
    "            yd_batch = yd_batch.to(device)\n",
    "\n",
    "            output, _ = model(X_valid)\n",
    "            loss = criterion(output, yd_batch[:, 0])\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            predicted_labels = torch.argmax(output, dim=1)\n",
    "            correct += (predicted_labels == yd_batch[:, 0]).sum().item()\n",
    "            total += yd_batch.size(0)\n",
    "\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_acc = correct / total * 100\n",
    "\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                x_batch, yd_batch, mask_batch = batch\n",
    "                X_valid = x_batch[mask_batch].reshape(x_batch.shape[0], -1, x_batch.shape[-1])\n",
    "                X_valid = X_valid.unsqueeze(1).to(device)\n",
    "                yd_batch = yd_batch.to(device)\n",
    "\n",
    "                output, _ = model(X_valid)\n",
    "                loss = criterion(output, yd_batch[:, 0])\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                predicted_labels = torch.argmax(output, dim=1)\n",
    "                val_correct += (predicted_labels == yd_batch[:, 0]).sum().item()\n",
    "                val_total += yd_batch.size(0)\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc = val_correct / val_total * 100\n",
    "\n",
    "        test_loss, test_correct, test_total = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                x_batch, yd_batch, mask_batch = batch\n",
    "                X_valid = x_batch[mask_batch].reshape(x_batch.shape[0], -1, x_batch.shape[-1])\n",
    "                X_valid = X_valid.unsqueeze(1).to(device)\n",
    "                yd_batch = yd_batch.to(device)\n",
    "\n",
    "                output, _ = model(X_valid)\n",
    "                loss = criterion(output, yd_batch[:, 0])\n",
    "                test_loss += loss.item()\n",
    "\n",
    "                predicted_labels = torch.argmax(output, dim=1)\n",
    "                test_correct += (predicted_labels == yd_batch[:, 0]).sum().item()\n",
    "                test_total += yd_batch.size(0)\n",
    "\n",
    "        test_loss /= len(test_loader)\n",
    "        test_acc = test_correct / test_total * 100\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            save_path = f\"./EEGNet0-Weibo-s{indSubj}-TEST.pth\"\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, save_path)\n",
    "\n",
    "            print(\"✅ Model saved.\")\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% | \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}% | \"\n",
    "              f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659ff0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "for indSubj in range(4):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    data = torch.load('./Dataset/eeg_data_zhou.pt', map_location=device)\n",
    "\n",
    "    X_tensor = data['X']\n",
    "    YD_tensor = data['YD']\n",
    "    padding_masks_tensor = data['mask']\n",
    "    channel_xy_tensor = data['channel_xy']\n",
    "\n",
    "    X_tensor_half = data['X_val']\n",
    "    YD_tensor_half = data['YD_val']\n",
    "    padding_masks_tensor_half = data['mask_val']\n",
    "\n",
    "    X_tensor_test = data['X_test']\n",
    "    YD_tensor_test = data['YD_test']\n",
    "    padding_masks_tensor_test = data['mask_test']\n",
    "\n",
    "    idx = YD_tensor[:, 4] == indSubj + 0\n",
    "    idx_half = YD_tensor_half[:, 4] == indSubj + 0\n",
    "    idx_test = YD_tensor_test[:, 4] == indSubj + 0\n",
    "\n",
    "    X_tensor = X_tensor[idx]\n",
    "    YD_tensor = YD_tensor[idx]\n",
    "    padding_masks_tensor = padding_masks_tensor[idx]\n",
    "\n",
    "    X_tensor_half = X_tensor_half[idx_half]\n",
    "    YD_tensor_half = YD_tensor_half[idx_half]\n",
    "    padding_masks_tensor_half = padding_masks_tensor_half[idx_half]\n",
    "\n",
    "    X_tensor_test = X_tensor_test[idx_test]\n",
    "    YD_tensor_test = YD_tensor_test[idx_test]\n",
    "    padding_masks_tensor_test = padding_masks_tensor_test[idx_test]\n",
    "\n",
    "    dataset = MultisourceDataset(X_tensor, YD_tensor, padding_masks_tensor)\n",
    "    dataloader = DataLoader(dataset, batch_size=10, shuffle=True)\n",
    "    dataset_test = MultisourceDataset(X_tensor_half, YD_tensor_half, padding_masks_tensor_half)\n",
    "    dataloader_test = DataLoader(dataset_test, batch_size=10, shuffle=True)\n",
    "    dataset_test0 = MultisourceDataset(X_tensor_test, YD_tensor_test, padding_masks_tensor_test)\n",
    "    dataloader_test0 = DataLoader(dataset_test0, batch_size=10, shuffle=True)\n",
    "\n",
    "    seed = 42\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    out_channels = 14\n",
    "    num_classes = 3\n",
    "    learning_rate = 0.0005\n",
    "    num_epochs = 500\n",
    "\n",
    "    model = EEGNet0(nb_classes=num_classes, Chans=out_channels, Samples=841).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "\n",
    "    train_loader = DataLoader(dataloader.dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(dataloader_test.dataset, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(dataloader_test0.dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    best_val_acc = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        correct, total, running_loss = 0, 0, 0\n",
    "        for batch in train_loader:\n",
    "            x_batch, yd_batch, mask_batch = batch\n",
    "            X_valid = x_batch[mask_batch].reshape(x_batch.shape[0], -1, x_batch.shape[-1])\n",
    "            X_valid = X_valid.unsqueeze(1).to(device)\n",
    "            yd_batch = yd_batch.to(device)\n",
    "\n",
    "            output, _ = model(X_valid)\n",
    "            loss = criterion(output, yd_batch[:, 0])\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            predicted_labels = torch.argmax(output, dim=1)\n",
    "            correct += (predicted_labels == yd_batch[:, 0]).sum().item()\n",
    "            total += yd_batch.size(0)\n",
    "\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_acc = correct / total * 100\n",
    "\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                x_batch, yd_batch, mask_batch = batch\n",
    "                X_valid = x_batch[mask_batch].reshape(x_batch.shape[0], -1, x_batch.shape[-1])\n",
    "                X_valid = X_valid.unsqueeze(1).to(device)\n",
    "                yd_batch = yd_batch.to(device)\n",
    "\n",
    "                output, _ = model(X_valid)\n",
    "                loss = criterion(output, yd_batch[:, 0])\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                predicted_labels = torch.argmax(output, dim=1)\n",
    "                val_correct += (predicted_labels == yd_batch[:, 0]).sum().item()\n",
    "                val_total += yd_batch.size(0)\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc = val_correct / val_total * 100\n",
    "\n",
    "        test_loss, test_correct, test_total = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                x_batch, yd_batch, mask_batch = batch\n",
    "                X_valid = x_batch[mask_batch].reshape(x_batch.shape[0], -1, x_batch.shape[-1])\n",
    "                X_valid = X_valid.unsqueeze(1).to(device)\n",
    "                yd_batch = yd_batch.to(device)\n",
    "\n",
    "                output, _ = model(X_valid)\n",
    "                loss = criterion(output, yd_batch[:, 0])\n",
    "                test_loss += loss.item()\n",
    "\n",
    "                predicted_labels = torch.argmax(output, dim=1)\n",
    "                test_correct += (predicted_labels == yd_batch[:, 0]).sum().item()\n",
    "                test_total += yd_batch.size(0)\n",
    "\n",
    "        test_loss /= len(test_loader)\n",
    "        test_acc = test_correct / test_total * 100\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            save_path = f\"./EEGNet0-Zhou-s{indSubj}-TEST.pth\"\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, save_path)\n",
    "\n",
    "            print(\"✅ Model saved.\")\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% | \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}% | \"\n",
    "              f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913639bb",
   "metadata": {},
   "source": [
    "TRAIN LOOP + AUGMENT (NOISE ADDITION/RECOMBINATION IN TIME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b74a585",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "for indSubj in range(9):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    data = torch.load('./Dataset/eeg_data_bnci.pt', map_location=device)\n",
    "\n",
    "    X_tensor = data['X']\n",
    "    YD_tensor = data['YD']\n",
    "    padding_masks_tensor = data['mask']\n",
    "    channel_xy_tensor = data['channel_xy']\n",
    "\n",
    "    X_tensor_half = data['X_val']\n",
    "    YD_tensor_half = data['YD_val']\n",
    "    padding_masks_tensor_half = data['mask_val']\n",
    "\n",
    "    X_tensor_test = data['X_test']\n",
    "    YD_tensor_test = data['YD_test']\n",
    "    padding_masks_tensor_test = data['mask_test']\n",
    "\n",
    "    idx = YD_tensor[:, 4] == indSubj + 4\n",
    "    idx_half = YD_tensor_half[:, 4] == indSubj + 4\n",
    "    idx_test = YD_tensor_test[:, 4] == indSubj + 4\n",
    "\n",
    "    X_tensor = X_tensor[idx]\n",
    "    YD_tensor = YD_tensor[idx]\n",
    "    padding_masks_tensor = padding_masks_tensor[idx]\n",
    "\n",
    "    aug_X, aug_YD, aug_mask = add_gaussian_noise_classwise(X_tensor, YD_tensor, padding_masks_tensor)\n",
    "    # aug_X, aug_YD, aug_mask = recombination_in_time(\n",
    "    #     X_tensor, YD_tensor, padding_masks_tensor,\n",
    "    #     n_chunks=5, output_length=X_tensor.shape[2]\n",
    "    # )\n",
    "\n",
    "    if aug_X is not None:\n",
    "        X_tensor = torch.cat([X_tensor, aug_X], dim=0)\n",
    "        YD_tensor = torch.cat([YD_tensor, aug_YD], dim=0)\n",
    "        padding_masks_tensor = torch.cat([padding_masks_tensor, aug_mask], dim=0)\n",
    "\n",
    "    X_tensor_half = X_tensor_half[idx_half]\n",
    "    YD_tensor_half = YD_tensor_half[idx_half]\n",
    "    padding_masks_tensor_half = padding_masks_tensor_half[idx_half]\n",
    "\n",
    "    X_tensor_test = X_tensor_test[idx_test]\n",
    "    YD_tensor_test = YD_tensor_test[idx_test]\n",
    "    padding_masks_tensor_test = padding_masks_tensor_test[idx_test]\n",
    "\n",
    "    dataset = MultisourceDataset(X_tensor, YD_tensor, padding_masks_tensor)\n",
    "    dataloader = DataLoader(dataset, batch_size=10, shuffle=True)\n",
    "    dataset_test = MultisourceDataset(X_tensor_half, YD_tensor_half, padding_masks_tensor_half)\n",
    "    dataloader_test = DataLoader(dataset_test, batch_size=10, shuffle=True)\n",
    "    dataset_test0 = MultisourceDataset(X_tensor_test, YD_tensor_test, padding_masks_tensor_test)\n",
    "    dataloader_test0 = DataLoader(dataset_test0, batch_size=10, shuffle=True)\n",
    "\n",
    "    seed = 42\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    out_channels = 22\n",
    "    num_classes = 3\n",
    "    learning_rate = 0.0005\n",
    "    num_epochs = 500\n",
    "\n",
    "    model = EEGNet0(nb_classes=num_classes, Chans=out_channels, Samples=841).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "\n",
    "    train_loader = DataLoader(dataloader.dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(dataloader_test.dataset, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(dataloader_test0.dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    best_val_acc = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        correct, total, running_loss = 0, 0, 0\n",
    "        for batch in train_loader:\n",
    "            x_batch, yd_batch, mask_batch = batch\n",
    "            X_valid = x_batch[mask_batch].reshape(x_batch.shape[0], -1, x_batch.shape[-1])\n",
    "            X_valid = X_valid.unsqueeze(1).to(device)\n",
    "            yd_batch = yd_batch.to(device)\n",
    "\n",
    "            output, _ = model(X_valid)\n",
    "            loss = criterion(output, yd_batch[:, 0])\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            predicted_labels = torch.argmax(output, dim=1)\n",
    "            correct += (predicted_labels == yd_batch[:, 0]).sum().item()\n",
    "            total += yd_batch.size(0)\n",
    "\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_acc = correct / total * 100\n",
    "\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                x_batch, yd_batch, mask_batch = batch\n",
    "                X_valid = x_batch[mask_batch].reshape(x_batch.shape[0], -1, x_batch.shape[-1])\n",
    "                X_valid = X_valid.unsqueeze(1).to(device)\n",
    "                yd_batch = yd_batch.to(device)\n",
    "\n",
    "                output, _ = model(X_valid)\n",
    "                loss = criterion(output, yd_batch[:, 0])\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                predicted_labels = torch.argmax(output, dim=1)\n",
    "                val_correct += (predicted_labels == yd_batch[:, 0]).sum().item()\n",
    "                val_total += yd_batch.size(0)\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc = val_correct / val_total * 100\n",
    "\n",
    "        test_loss, test_correct, test_total = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                x_batch, yd_batch, mask_batch = batch\n",
    "                X_valid = x_batch[mask_batch].reshape(x_batch.shape[0], -1, x_batch.shape[-1])\n",
    "                X_valid = X_valid.unsqueeze(1).to(device)\n",
    "                yd_batch = yd_batch.to(device)\n",
    "\n",
    "                output, _ = model(X_valid)\n",
    "                loss = criterion(output, yd_batch[:, 0])\n",
    "                test_loss += loss.item()\n",
    "\n",
    "                predicted_labels = torch.argmax(output, dim=1)\n",
    "                test_correct += (predicted_labels == yd_batch[:, 0]).sum().item()\n",
    "                test_total += yd_batch.size(0)\n",
    "\n",
    "        test_loss /= len(test_loader)\n",
    "        test_acc = test_correct / test_total * 100\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            save_path = f\"./EEGNet0-BNCI-s{indSubj}-TEST.pth\"\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, save_path)\n",
    "\n",
    "            print(\"✅ Model saved.\")\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% | \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}% | \"\n",
    "              f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f577bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "for indSubj in range(10):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    data = torch.load('./Dataset/eeg_data_weibo.pt', map_location=device)\n",
    "\n",
    "    X_tensor = data['X']\n",
    "    YD_tensor = data['YD']\n",
    "    padding_masks_tensor = data['mask']\n",
    "    channel_xy_tensor = data['channel_xy']\n",
    "\n",
    "    X_tensor_half = data['X_val']\n",
    "    YD_tensor_half = data['YD_val']\n",
    "    padding_masks_tensor_half = data['mask_val']\n",
    "\n",
    "    X_tensor_test = data['X_test']\n",
    "    YD_tensor_test = data['YD_test']\n",
    "    padding_masks_tensor_test = data['mask_test']\n",
    "\n",
    "    idx = YD_tensor[:, 4] == indSubj + 13\n",
    "    idx_half = YD_tensor_half[:, 4] == indSubj + 13\n",
    "    idx_test = YD_tensor_test[:, 4] == indSubj + 13\n",
    "\n",
    "    X_tensor = X_tensor[idx]\n",
    "    YD_tensor = YD_tensor[idx]\n",
    "    padding_masks_tensor = padding_masks_tensor[idx]\n",
    "\n",
    "    aug_X, aug_YD, aug_mask = add_gaussian_noise_classwise(X_tensor, YD_tensor, padding_masks_tensor)\n",
    "    # aug_X, aug_YD, aug_mask = recombination_in_time(\n",
    "    #     X_tensor, YD_tensor, padding_masks_tensor,\n",
    "    #     n_chunks=5, output_length=X_tensor.shape[2]\n",
    "    # )\n",
    "\n",
    "    if aug_X is not None:\n",
    "        X_tensor = torch.cat([X_tensor, aug_X], dim=0)\n",
    "        YD_tensor = torch.cat([YD_tensor, aug_YD], dim=0)\n",
    "        padding_masks_tensor = torch.cat([padding_masks_tensor, aug_mask], dim=0)\n",
    "\n",
    "    X_tensor_half = X_tensor_half[idx_half]\n",
    "    YD_tensor_half = YD_tensor_half[idx_half]\n",
    "    padding_masks_tensor_half = padding_masks_tensor_half[idx_half]\n",
    "\n",
    "    X_tensor_test = X_tensor_test[idx_test]\n",
    "    YD_tensor_test = YD_tensor_test[idx_test]\n",
    "    padding_masks_tensor_test = padding_masks_tensor_test[idx_test]\n",
    "\n",
    "    dataset = MultisourceDataset(X_tensor, YD_tensor, padding_masks_tensor)\n",
    "    dataloader = DataLoader(dataset, batch_size=10, shuffle=True)\n",
    "    dataset_test = MultisourceDataset(X_tensor_half, YD_tensor_half, padding_masks_tensor_half)\n",
    "    dataloader_test = DataLoader(dataset_test, batch_size=10, shuffle=True)\n",
    "    dataset_test0 = MultisourceDataset(X_tensor_test, YD_tensor_test, padding_masks_tensor_test)\n",
    "    dataloader_test0 = DataLoader(dataset_test0, batch_size=10, shuffle=True)\n",
    "\n",
    "    seed = 42\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    out_channels = 60\n",
    "    num_classes = 3\n",
    "    learning_rate = 0.0005\n",
    "    num_epochs = 500\n",
    "\n",
    "    model = EEGNet0(nb_classes=num_classes, Chans=out_channels, Samples=841).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "\n",
    "    train_loader = DataLoader(dataloader.dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(dataloader_test.dataset, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(dataloader_test0.dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    best_val_acc = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        correct, total, running_loss = 0, 0, 0\n",
    "        for batch in train_loader:\n",
    "            x_batch, yd_batch, mask_batch = batch\n",
    "            X_valid = x_batch[mask_batch].reshape(x_batch.shape[0], -1, x_batch.shape[-1])\n",
    "            X_valid = X_valid.unsqueeze(1).to(device)\n",
    "            yd_batch = yd_batch.to(device)\n",
    "\n",
    "            output, _ = model(X_valid)\n",
    "            loss = criterion(output, yd_batch[:, 0])\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            predicted_labels = torch.argmax(output, dim=1)\n",
    "            correct += (predicted_labels == yd_batch[:, 0]).sum().item()\n",
    "            total += yd_batch.size(0)\n",
    "\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_acc = correct / total * 100\n",
    "\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                x_batch, yd_batch, mask_batch = batch\n",
    "                X_valid = x_batch[mask_batch].reshape(x_batch.shape[0], -1, x_batch.shape[-1])\n",
    "                X_valid = X_valid.unsqueeze(1).to(device)\n",
    "                yd_batch = yd_batch.to(device)\n",
    "\n",
    "                output, _ = model(X_valid)\n",
    "                loss = criterion(output, yd_batch[:, 0])\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                predicted_labels = torch.argmax(output, dim=1)\n",
    "                val_correct += (predicted_labels == yd_batch[:, 0]).sum().item()\n",
    "                val_total += yd_batch.size(0)\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc = val_correct / val_total * 100\n",
    "\n",
    "        test_loss, test_correct, test_total = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                x_batch, yd_batch, mask_batch = batch\n",
    "                X_valid = x_batch[mask_batch].reshape(x_batch.shape[0], -1, x_batch.shape[-1])\n",
    "                X_valid = X_valid.unsqueeze(1).to(device)\n",
    "                yd_batch = yd_batch.to(device)\n",
    "\n",
    "                output, _ = model(X_valid)\n",
    "                loss = criterion(output, yd_batch[:, 0])\n",
    "                test_loss += loss.item()\n",
    "\n",
    "                predicted_labels = torch.argmax(output, dim=1)\n",
    "                test_correct += (predicted_labels == yd_batch[:, 0]).sum().item()\n",
    "                test_total += yd_batch.size(0)\n",
    "\n",
    "        test_loss /= len(test_loader)\n",
    "        test_acc = test_correct / test_total * 100\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            save_path = f\"./EEGNet0-Weibo-s{indSubj}-TEST.pth\"\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, save_path)\n",
    "\n",
    "            print(\"✅ Model saved.\")\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% | \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}% | \"\n",
    "              f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f7cb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "for indSubj in range(4):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    data = torch.load('./Dataset/eeg_data_zhou.pt', map_location=device)\n",
    "\n",
    "    X_tensor = data['X']\n",
    "    YD_tensor = data['YD']\n",
    "    padding_masks_tensor = data['mask']\n",
    "    channel_xy_tensor = data['channel_xy']\n",
    "\n",
    "    X_tensor_half = data['X_val']\n",
    "    YD_tensor_half = data['YD_val']\n",
    "    padding_masks_tensor_half = data['mask_val']\n",
    "\n",
    "    X_tensor_test = data['X_test']\n",
    "    YD_tensor_test = data['YD_test']\n",
    "    padding_masks_tensor_test = data['mask_test']\n",
    "\n",
    "    idx = YD_tensor[:, 4] == indSubj + 0\n",
    "    idx_half = YD_tensor_half[:, 4] == indSubj + 0\n",
    "    idx_test = YD_tensor_test[:, 4] == indSubj + 0\n",
    "\n",
    "    X_tensor = X_tensor[idx]\n",
    "    YD_tensor = YD_tensor[idx]\n",
    "    padding_masks_tensor = padding_masks_tensor[idx]\n",
    "\n",
    "    aug_X, aug_YD, aug_mask = add_gaussian_noise_classwise(X_tensor, YD_tensor, padding_masks_tensor)\n",
    "    # aug_X, aug_YD, aug_mask = recombination_in_time(\n",
    "    #     X_tensor, YD_tensor, padding_masks_tensor,\n",
    "    #     n_chunks=5, output_length=X_tensor.shape[2]\n",
    "    # )\n",
    "\n",
    "    if aug_X is not None:\n",
    "        X_tensor = torch.cat([X_tensor, aug_X], dim=0)\n",
    "        YD_tensor = torch.cat([YD_tensor, aug_YD], dim=0)\n",
    "        padding_masks_tensor = torch.cat([padding_masks_tensor, aug_mask], dim=0)\n",
    "\n",
    "    X_tensor_half = X_tensor_half[idx_half]\n",
    "    YD_tensor_half = YD_tensor_half[idx_half]\n",
    "    padding_masks_tensor_half = padding_masks_tensor_half[idx_half]\n",
    "\n",
    "    X_tensor_test = X_tensor_test[idx_test]\n",
    "    YD_tensor_test = YD_tensor_test[idx_test]\n",
    "    padding_masks_tensor_test = padding_masks_tensor_test[idx_test]\n",
    "\n",
    "    dataset = MultisourceDataset(X_tensor, YD_tensor, padding_masks_tensor)\n",
    "    dataloader = DataLoader(dataset, batch_size=10, shuffle=True)\n",
    "    dataset_test = MultisourceDataset(X_tensor_half, YD_tensor_half, padding_masks_tensor_half)\n",
    "    dataloader_test = DataLoader(dataset_test, batch_size=10, shuffle=True)\n",
    "    dataset_test0 = MultisourceDataset(X_tensor_test, YD_tensor_test, padding_masks_tensor_test)\n",
    "    dataloader_test0 = DataLoader(dataset_test0, batch_size=10, shuffle=True)\n",
    "\n",
    "    seed = 42\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    out_channels = 14\n",
    "    num_classes = 3\n",
    "    learning_rate = 0.0005\n",
    "    num_epochs = 500\n",
    "\n",
    "    model = EEGNet0(nb_classes=num_classes, Chans=out_channels, Samples=841).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "\n",
    "    train_loader = DataLoader(dataloader.dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(dataloader_test.dataset, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(dataloader_test0.dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    best_val_acc = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        correct, total, running_loss = 0, 0, 0\n",
    "        for batch in train_loader:\n",
    "            x_batch, yd_batch, mask_batch = batch\n",
    "            X_valid = x_batch[mask_batch].reshape(x_batch.shape[0], -1, x_batch.shape[-1])\n",
    "            X_valid = X_valid.unsqueeze(1).to(device)\n",
    "            yd_batch = yd_batch.to(device)\n",
    "\n",
    "            output, _ = model(X_valid)\n",
    "            loss = criterion(output, yd_batch[:, 0])\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            predicted_labels = torch.argmax(output, dim=1)\n",
    "            correct += (predicted_labels == yd_batch[:, 0]).sum().item()\n",
    "            total += yd_batch.size(0)\n",
    "\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_acc = correct / total * 100\n",
    "\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                x_batch, yd_batch, mask_batch = batch\n",
    "                X_valid = x_batch[mask_batch].reshape(x_batch.shape[0], -1, x_batch.shape[-1])\n",
    "                X_valid = X_valid.unsqueeze(1).to(device)\n",
    "                yd_batch = yd_batch.to(device)\n",
    "\n",
    "                output, _ = model(X_valid)\n",
    "                loss = criterion(output, yd_batch[:, 0])\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                predicted_labels = torch.argmax(output, dim=1)\n",
    "                val_correct += (predicted_labels == yd_batch[:, 0]).sum().item()\n",
    "                val_total += yd_batch.size(0)\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc = val_correct / val_total * 100\n",
    "\n",
    "        test_loss, test_correct, test_total = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                x_batch, yd_batch, mask_batch = batch\n",
    "                X_valid = x_batch[mask_batch].reshape(x_batch.shape[0], -1, x_batch.shape[-1])\n",
    "                X_valid = X_valid.unsqueeze(1).to(device)\n",
    "                yd_batch = yd_batch.to(device)\n",
    "\n",
    "                output, _ = model(X_valid)\n",
    "                loss = criterion(output, yd_batch[:, 0])\n",
    "                test_loss += loss.item()\n",
    "\n",
    "                predicted_labels = torch.argmax(output, dim=1)\n",
    "                test_correct += (predicted_labels == yd_batch[:, 0]).sum().item()\n",
    "                test_total += yd_batch.size(0)\n",
    "\n",
    "        test_loss /= len(test_loader)\n",
    "        test_acc = test_correct / test_total * 100\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            save_path = f\"./EEGNet0-Zhou-s{indSubj}-TEST.pth\"\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, save_path)\n",
    "\n",
    "            print(\"✅ Model saved.\")\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% | \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}% | \"\n",
    "              f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b99c048",
   "metadata": {},
   "source": [
    "TRAIN LOOP MULTI-SOURCE DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7892c3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "data = torch.load('./Dataset/eeg_data_200.pt', map_location=device)\n",
    "\n",
    "X_tensor = data['X']\n",
    "YD_tensor = data['YD']\n",
    "padding_masks_tensor = data['mask']\n",
    "channel_xy_tensor = data['channel_xy']\n",
    "\n",
    "X_tensor_half = data['X_val']\n",
    "YD_tensor_half = data['YD_val']\n",
    "padding_masks_tensor_half = data['mask_val']\n",
    "\n",
    "X_tensor_test = data['X_test']\n",
    "YD_tensor_test = data['YD_test']\n",
    "padding_masks_tensor_test = data['mask_test']\n",
    "\n",
    "dataset = MultisourceDataset(X_tensor, YD_tensor, padding_masks_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=10, shuffle=True)\n",
    "dataset_test = MultisourceDataset(X_tensor_half, YD_tensor_half, padding_masks_tensor_half)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=10, shuffle=True)\n",
    "dataset_test0 = MultisourceDataset(X_tensor_test, YD_tensor_test, padding_masks_tensor_test)\n",
    "dataloader_test0 = DataLoader(dataset_test0, batch_size=10, shuffle=True)\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "out_channels = 60\n",
    "num_classes = 3\n",
    "learning_rate = 0.0005\n",
    "num_epochs = 500\n",
    "\n",
    "model = EEGNet0(nb_classes=num_classes, Chans=out_channels, Samples=841).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "\n",
    "train_loader = DataLoader(dataloader.dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(dataloader_test.dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(dataloader_test0.dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "best_val_acc = 0\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    correct, total, running_loss = 0, 0, 0\n",
    "    for batch in train_loader:\n",
    "        x_batch, yd_batch, mask_batch = batch\n",
    "        # X_valid = x_batch[mask_batch].reshape(x_batch.shape[0], -1, x_batch.shape[-1])\n",
    "        X_valid = x_batch\n",
    "        X_valid = X_valid.unsqueeze(1).to(device)\n",
    "        yd_batch = yd_batch.to(device)\n",
    "\n",
    "        output, _ = model(X_valid)\n",
    "        loss = criterion(output, yd_batch[:, 0])\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        predicted_labels = torch.argmax(output, dim=1)\n",
    "        correct += (predicted_labels == yd_batch[:, 0]).sum().item()\n",
    "        total += yd_batch.size(0)\n",
    "\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_acc = correct / total * 100\n",
    "\n",
    "    model.eval()\n",
    "    val_loss, val_correct, val_total = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            x_batch, yd_batch, mask_batch = batch\n",
    "            # X_valid = x_batch[mask_batch].reshape(x_batch.shape[0], -1, x_batch.shape[-1])\n",
    "            X_valid = x_batch\n",
    "\n",
    "\n",
    "            X_valid = X_valid.unsqueeze(1).to(device)\n",
    "            yd_batch = yd_batch.to(device)\n",
    "\n",
    "            output, _ = model(X_valid)\n",
    "            loss = criterion(output, yd_batch[:, 0])\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            predicted_labels = torch.argmax(output, dim=1)\n",
    "            val_correct += (predicted_labels == yd_batch[:, 0]).sum().item()\n",
    "            val_total += yd_batch.size(0)\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_acc = val_correct / val_total * 100\n",
    "\n",
    "    test_loss, test_correct, test_total = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            x_batch, yd_batch, mask_batch = batch\n",
    "            # X_valid = x_batch[mask_batch].reshape(x_batch.shape[0], -1, x_batch.shape[-1])\n",
    "            X_valid = x_batch\n",
    "\n",
    "\n",
    "            X_valid = X_valid.unsqueeze(1).to(device)\n",
    "            yd_batch = yd_batch.to(device)\n",
    "\n",
    "            output, _ = model(X_valid)\n",
    "            loss = criterion(output, yd_batch[:, 0])\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            predicted_labels = torch.argmax(output, dim=1)\n",
    "            test_correct += (predicted_labels == yd_batch[:, 0]).sum().item()\n",
    "            test_total += yd_batch.size(0)\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    test_acc = test_correct / test_total * 100\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        save_path = f\"./EEGNet0-X-TEST.pth\"\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "        }, save_path)\n",
    "\n",
    "        print(\"✅ Model saved.\")\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "            f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% | \"\n",
    "            f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}% | \"\n",
    "            f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbb3f08",
   "metadata": {},
   "source": [
    "PERFORMANCE EVALUATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bcdc2868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics by Domain (Each with its own EEGNet0 model):\n",
      "\n",
      "- Domain 13 (Model 0):\n",
      "    - Accuracy: 0.5612\n",
      "    - Class 0: F1 Score = 0.7719\n",
      "    - Class 1: F1 Score = 0.5185\n",
      "    - Class 2: F1 Score = 0.4138\n",
      "    - Misclassification Breakdown (rows = true class):\n",
      "        Class 0 misclassified as: 23.5% → class 1, 11.8% → class 2\n",
      "        Class 1 misclassified as: 2.9% → class 0, 37.1% → class 2\n",
      "        Class 2 misclassified as: 0.0% → class 0, 58.6% → class 1\n",
      "\n",
      "- Domain 14 (Model 1):\n",
      "    - Accuracy: 0.3882\n",
      "    - Class 0: F1 Score = 0.4675\n",
      "    - Class 1: F1 Score = 0.4000\n",
      "    - Class 2: F1 Score = 0.1818\n",
      "    - Misclassification Breakdown (rows = true class):\n",
      "        Class 0 misclassified as: 35.5% → class 1, 6.5% → class 2\n",
      "        Class 1 misclassified as: 48.3% → class 0, 10.3% → class 2\n",
      "        Class 2 misclassified as: 56.0% → class 0, 32.0% → class 1\n",
      "\n",
      "- Domain 15 (Model 2):\n",
      "    - Accuracy: 0.3298\n",
      "    - Class 0: F1 Score = 0.3158\n",
      "    - Class 1: F1 Score = 0.3721\n",
      "    - Class 2: F1 Score = 0.2667\n",
      "    - Misclassification Breakdown (rows = true class):\n",
      "        Class 0 misclassified as: 58.3% → class 1, 16.7% → class 2\n",
      "        Class 1 misclassified as: 33.3% → class 0, 7.4% → class 2\n",
      "        Class 2 misclassified as: 9.7% → class 0, 71.0% → class 1\n",
      "\n",
      "- Domain 16 (Model 3):\n",
      "    - Accuracy: 0.6162\n",
      "    - Class 0: F1 Score = 0.9180\n",
      "    - Class 1: F1 Score = 0.4928\n",
      "    - Class 2: F1 Score = 0.4706\n",
      "    - Misclassification Breakdown (rows = true class):\n",
      "        Class 0 misclassified as: 3.2% → class 1, 6.5% → class 2\n",
      "        Class 1 misclassified as: 2.9% → class 0, 48.6% → class 2\n",
      "        Class 2 misclassified as: 3.0% → class 0, 48.5% → class 1\n",
      "\n",
      "- Domain 17 (Model 4):\n",
      "    - Accuracy: 0.5000\n",
      "    - Class 0: F1 Score = 0.8214\n",
      "    - Class 1: F1 Score = 0.3582\n",
      "    - Class 2: F1 Score = 0.3607\n",
      "    - Misclassification Breakdown (rows = true class):\n",
      "        Class 0 misclassified as: 14.3% → class 1, 3.6% → class 2\n",
      "        Class 1 misclassified as: 9.7% → class 0, 51.6% → class 2\n",
      "        Class 2 misclassified as: 6.1% → class 0, 60.6% → class 1\n",
      "\n",
      "- Domain 18 (Model 5):\n",
      "    - Accuracy: 0.4699\n",
      "    - Class 0: F1 Score = 0.4186\n",
      "    - Class 1: F1 Score = 0.4571\n",
      "    - Class 2: F1 Score = 0.5283\n",
      "    - Misclassification Breakdown (rows = true class):\n",
      "        Class 0 misclassified as: 47.6% → class 1, 9.5% → class 2\n",
      "        Class 1 misclassified as: 31.0% → class 0, 13.8% → class 2\n",
      "        Class 2 misclassified as: 12.1% → class 0, 45.5% → class 1\n",
      "\n",
      "- Domain 19 (Model 6):\n",
      "    - Accuracy: 0.7931\n",
      "    - Class 0: F1 Score = 0.6809\n",
      "    - Class 1: F1 Score = 0.8333\n",
      "    - Class 2: F1 Score = 0.8358\n",
      "    - Misclassification Breakdown (rows = true class):\n",
      "        Class 0 misclassified as: 15.4% → class 1, 23.1% → class 2\n",
      "        Class 1 misclassified as: 10.0% → class 0, 6.7% → class 2\n",
      "        Class 2 misclassified as: 6.5% → class 0, 3.2% → class 1\n",
      "\n",
      "- Domain 20 (Model 7):\n",
      "    - Accuracy: 0.8587\n",
      "    - Class 0: F1 Score = 0.8235\n",
      "    - Class 1: F1 Score = 0.8276\n",
      "    - Class 2: F1 Score = 0.9067\n",
      "    - Misclassification Breakdown (rows = true class):\n",
      "        Class 0 misclassified as: 14.3% → class 1, 10.7% → class 2\n",
      "        Class 1 misclassified as: 6.7% → class 0, 13.3% → class 2\n",
      "        Class 2 misclassified as: 0.0% → class 0, 0.0% → class 1\n",
      "\n",
      "- Domain 21 (Model 8):\n",
      "    - Accuracy: 0.7340\n",
      "    - Class 0: F1 Score = 0.9259\n",
      "    - Class 1: F1 Score = 0.6557\n",
      "    - Class 2: F1 Score = 0.6575\n",
      "    - Misclassification Breakdown (rows = true class):\n",
      "        Class 0 misclassified as: 0.0% → class 1, 10.7% → class 2\n",
      "        Class 1 misclassified as: 0.0% → class 0, 37.5% → class 2\n",
      "        Class 2 misclassified as: 2.9% → class 0, 26.5% → class 1\n",
      "\n",
      "- Domain 22 (Model 9):\n",
      "    - Accuracy: 0.5275\n",
      "    - Class 0: F1 Score = 0.6471\n",
      "    - Class 1: F1 Score = 0.5588\n",
      "    - Class 2: F1 Score = 0.3043\n",
      "    - Misclassification Breakdown (rows = true class):\n",
      "        Class 0 misclassified as: 24.3% → class 1, 16.2% → class 2\n",
      "        Class 1 misclassified as: 7.4% → class 0, 22.2% → class 2\n",
      "        Class 2 misclassified as: 25.9% → class 0, 48.1% → class 1\n",
      "\n",
      "Overall Metrics:\n",
      "    - Overall Accuracy: 0.5792\n",
      "    - Class 0: F1 Score = 0.6760\n",
      "    - Class 1: F1 Score = 0.5353\n",
      "    - Class 2: F1 Score = 0.5354\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "data = torch.load('./Dataset/eeg_data_weibo.pt', map_location=device)\n",
    "\n",
    "X_tensor = data['X']\n",
    "YD_tensor = data['YD']\n",
    "padding_masks_tensor = data['mask']\n",
    "channel_xy_tensor = data['channel_xy']\n",
    "\n",
    "X_tensor_half = data['X_val']\n",
    "YD_tensor_half = data['YD_val']\n",
    "padding_masks_tensor_half = data['mask_val']\n",
    "\n",
    "X_tensor_test = data['X_test']\n",
    "YD_tensor_test = data['YD_test']\n",
    "padding_masks_tensor_test = data['mask_test']\n",
    "\n",
    "dataset = MultisourceDataset(X_tensor, YD_tensor, padding_masks_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=10, shuffle=True)\n",
    "dataset_test = MultisourceDataset(X_tensor_half, YD_tensor_half, padding_masks_tensor_half)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=10, shuffle=True)\n",
    "dataset_test0 = MultisourceDataset(X_tensor_test, YD_tensor_test, padding_masks_tensor_test)\n",
    "dataloader_test0 = DataLoader(dataset_test0, batch_size=10, shuffle=True)\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# === Seeding ===\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# === Hyperparameters ===\n",
    "Chans = 60  # Input EEG channels\n",
    "Samples = 841  # Number of time samples\n",
    "num_classes = 3\n",
    "learning_rate = 0.0005\n",
    "num_epochs = 500\n",
    "\n",
    "# === Test Loader ===\n",
    "test_loader = DataLoader(dataloader_test0.dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# === Global Containers for Overall Metrics ===\n",
    "overall_preds = []\n",
    "overall_targets = []\n",
    "\n",
    "# === Loop over last 10 domains (13 to 22 in dataset) ===\n",
    "print(\"Metrics by Domain (Each with its own EEGNet0 model):\")\n",
    "for model_id in range(10):\n",
    "    dataset_domain_id = model_id + 13\n",
    "\n",
    "    # === Initialize Model ===\n",
    "    model = EEGNet0(nb_classes=num_classes, Chans=Chans, Samples=Samples).to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # === Load Checkpoint ===\n",
    "    save_path = f\"./BaselineModel/EEGNet0-Weibo-s{model_id}-rt-500.pth\"\n",
    "    if not os.path.exists(save_path):\n",
    "        print(f\"⚠️  Checkpoint not found for model {model_id}: {save_path}\")\n",
    "        continue\n",
    "\n",
    "    checkpoint = torch.load(save_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    # === Containers for Domain-Specific Evaluation ===\n",
    "    y_true_domain = []\n",
    "    y_pred_domain = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            x_batch, yd_batch, mask_batch = batch\n",
    "            domains = yd_batch[:, 4].cpu().numpy()\n",
    "\n",
    "            # Filter only samples from dataset domain `model_id + 13`\n",
    "            mask = (domains == dataset_domain_id)\n",
    "            if not np.any(mask):\n",
    "                continue\n",
    "\n",
    "            x_domain = x_batch[mask]\n",
    "            yd_domain = yd_batch[mask]\n",
    "            mask_domain = mask_batch[mask]\n",
    "\n",
    "            if x_domain.size(0) == 0:\n",
    "                continue\n",
    "\n",
    "            # Apply masking and reshape: (B, 1, Chans, Samples)\n",
    "            X_valid = x_domain[mask_domain].reshape(x_domain.shape[0], -1, x_domain.shape[-1])\n",
    "            X_valid = X_valid.unsqueeze(1).to(device)\n",
    "\n",
    "            logits, _ = model(X_valid)\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "\n",
    "            targets = yd_domain[:, 0].cpu().numpy()\n",
    "            preds = predictions.cpu().numpy()\n",
    "\n",
    "            y_true_domain.extend(targets)\n",
    "            y_pred_domain.extend(preds)\n",
    "\n",
    "    # === Metrics for current domain ===\n",
    "    if not y_true_domain:\n",
    "        print(f\"\\n- Domain {dataset_domain_id} (Model {model_id}): No data.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n- Domain {dataset_domain_id} (Model {model_id}):\")\n",
    "    accuracy = accuracy_score(y_true_domain, y_pred_domain)\n",
    "    print(f\"    - Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    classes = sorted(set(y_true_domain))\n",
    "    f1_scores = f1_score(y_true_domain, y_pred_domain, average=None, labels=classes)\n",
    "    for cls, f1 in zip(classes, f1_scores):\n",
    "        print(f\"    - Class {cls}: F1 Score = {f1:.4f}\")\n",
    "\n",
    "    cm = confusion_matrix(y_true_domain, y_pred_domain, labels=classes)\n",
    "    cm_percent = cm.astype(float) / cm.sum(axis=1, keepdims=True)\n",
    "\n",
    "    print(f\"    - Misclassification Breakdown (rows = true class):\")\n",
    "    for i, cls in enumerate(classes):\n",
    "        breakdown = []\n",
    "        for j, pred_cls in enumerate(classes):\n",
    "            pct = cm_percent[i, j] * 100\n",
    "            if i == j:\n",
    "                continue\n",
    "            breakdown.append(f\"{pct:.1f}% → class {pred_cls}\")\n",
    "        if breakdown:\n",
    "            print(f\"        Class {cls} misclassified as: {', '.join(breakdown)}\")\n",
    "        else:\n",
    "            print(f\"        Class {cls} has no misclassifications.\")\n",
    "\n",
    "    # Accumulate for overall metrics\n",
    "    overall_preds.extend(y_pred_domain)\n",
    "    overall_targets.extend(y_true_domain)\n",
    "\n",
    "# === Print overall metrics ===\n",
    "if overall_preds:\n",
    "    overall_accuracy = accuracy_score(overall_targets, overall_preds)\n",
    "    all_classes = sorted(set(overall_targets))\n",
    "    f1_per_class = f1_score(overall_targets, overall_preds, average=None, labels=all_classes)\n",
    "\n",
    "    print(\"\\nOverall Metrics:\")\n",
    "    print(f\"    - Overall Accuracy: {overall_accuracy:.4f}\")\n",
    "    for cls, f1 in zip(all_classes, f1_per_class):\n",
    "        print(f\"    - Class {cls}: F1 Score = {f1:.4f}\")\n",
    "else:\n",
    "    print(\"\\nNo predictions were collected. Please check domain IDs or dataset.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16a629dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics by Domain (Using One Model):\n",
      "\n",
      "- Domain 0:\n",
      "    - Accuracy: 0.7800\n",
      "    - Class 0: F1 Score = 0.8000\n",
      "    - Class 1: F1 Score = 0.7872\n",
      "    - Class 2: F1 Score = 0.7586\n",
      "    - Misclassification Breakdown (rows = true class):\n",
      "        Class 0 misclassified as: 8.0% → class 1, 20.0% → class 2\n",
      "        Class 1 misclassified as: 2.0% → class 0, 24.0% → class 2\n",
      "        Class 2 misclassified as: 6.0% → class 0, 6.0% → class 1\n",
      "\n",
      "- Domain 1:\n",
      "    - Accuracy: 0.8800\n",
      "    - Class 0: F1 Score = 0.9899\n",
      "    - Class 1: F1 Score = 0.8317\n",
      "    - Class 2: F1 Score = 0.8200\n",
      "    - Misclassification Breakdown (rows = true class):\n",
      "        Class 0 misclassified as: 0.0% → class 1, 2.0% → class 2\n",
      "        Class 1 misclassified as: 0.0% → class 0, 16.0% → class 2\n",
      "        Class 2 misclassified as: 0.0% → class 0, 18.0% → class 1\n",
      "\n",
      "- Domain 2:\n",
      "    - Accuracy: 0.8200\n",
      "    - Class 0: F1 Score = 0.8257\n",
      "    - Class 1: F1 Score = 0.8261\n",
      "    - Class 2: F1 Score = 0.8081\n",
      "    - Misclassification Breakdown (rows = true class):\n",
      "        Class 0 misclassified as: 4.0% → class 1, 6.0% → class 2\n",
      "        Class 1 misclassified as: 12.0% → class 0, 12.0% → class 2\n",
      "        Class 2 misclassified as: 16.0% → class 0, 4.0% → class 1\n",
      "\n",
      "- Domain 3:\n",
      "    - Accuracy: 0.9000\n",
      "    - Class 0: F1 Score = 0.9293\n",
      "    - Class 1: F1 Score = 0.9159\n",
      "    - Class 2: F1 Score = 0.8511\n",
      "    - Misclassification Breakdown (rows = true class):\n",
      "        Class 0 misclassified as: 2.0% → class 1, 6.0% → class 2\n",
      "        Class 1 misclassified as: 0.0% → class 0, 2.0% → class 2\n",
      "        Class 2 misclassified as: 6.0% → class 0, 14.0% → class 1\n",
      "\n",
      "- Domain 4:\n",
      "    - Accuracy: 0.8843\n",
      "    - Class 0: F1 Score = 0.9452\n",
      "    - Class 1: F1 Score = 0.8120\n",
      "    - Class 2: F1 Score = 0.8889\n",
      "    - Misclassification Breakdown (rows = true class):\n",
      "        Class 0 misclassified as: 4.2% → class 1, 0.0% → class 2\n",
      "        Class 1 misclassified as: 6.9% → class 0, 18.1% → class 2\n",
      "        Class 2 misclassified as: 0.0% → class 0, 5.6% → class 1\n",
      "\n",
      "- Domain 5:\n",
      "    - Accuracy: 0.4120\n",
      "    - Class 0: F1 Score = 0.2128\n",
      "    - Class 1: F1 Score = 0.4151\n",
      "    - Class 2: F1 Score = 0.5140\n",
      "    - Misclassification Breakdown (rows = true class):\n",
      "        Class 0 misclassified as: 48.6% → class 1, 37.5% → class 2\n",
      "        Class 1 misclassified as: 6.9% → class 0, 47.2% → class 2\n",
      "        Class 2 misclassified as: 9.7% → class 0, 26.4% → class 1\n",
      "\n",
      "- Domain 6:\n",
      "    - Accuracy: 0.8935\n",
      "    - Class 0: F1 Score = 0.8828\n",
      "    - Class 1: F1 Score = 0.8824\n",
      "    - Class 2: F1 Score = 0.9139\n",
      "    - Misclassification Breakdown (rows = true class):\n",
      "        Class 0 misclassified as: 2.8% → class 1, 8.3% → class 2\n",
      "        Class 1 misclassified as: 11.1% → class 0, 5.6% → class 2\n",
      "        Class 2 misclassified as: 1.4% → class 0, 2.8% → class 1\n",
      "\n",
      "- Domain 7:\n",
      "    - Accuracy: 0.5370\n",
      "    - Class 0: F1 Score = 0.6494\n",
      "    - Class 1: F1 Score = 0.2128\n",
      "    - Class 2: F1 Score = 0.6087\n",
      "    - Misclassification Breakdown (rows = true class):\n",
      "        Class 0 misclassified as: 6.9% → class 1, 23.6% → class 2\n",
      "        Class 1 misclassified as: 31.9% → class 0, 54.2% → class 2\n",
      "        Class 2 misclassified as: 12.5% → class 0, 9.7% → class 1\n",
      "\n",
      "- Domain 8:\n",
      "    - Accuracy: 0.3981\n",
      "    - Class 0: F1 Score = 0.1176\n",
      "    - Class 1: F1 Score = 0.4904\n",
      "    - Class 2: F1 Score = 0.4317\n",
      "    - Misclassification Breakdown (rows = true class):\n",
      "        Class 0 misclassified as: 65.3% → class 1, 27.8% → class 2\n",
      "        Class 1 misclassified as: 5.6% → class 0, 23.6% → class 2\n",
      "        Class 2 misclassified as: 5.6% → class 0, 52.8% → class 1\n",
      "\n",
      "- Domain 9:\n",
      "    - Accuracy: 0.4954\n",
      "    - Class 0: F1 Score = 0.5444\n",
      "    - Class 1: F1 Score = 0.4274\n",
      "    - Class 2: F1 Score = 0.4889\n",
      "    - Misclassification Breakdown (rows = true class):\n",
      "        Class 0 misclassified as: 12.5% → class 1, 19.4% → class 2\n",
      "        Class 1 misclassified as: 43.1% → class 0, 22.2% → class 2\n",
      "        Class 2 misclassified as: 38.9% → class 0, 15.3% → class 1\n",
      "\n",
      "- Domain 10:\n",
      "    - Accuracy: 0.4213\n",
      "    - Class 0: F1 Score = 0.2885\n",
      "    - Class 1: F1 Score = 0.4774\n",
      "    - Class 2: F1 Score = 0.4509\n",
      "    - Misclassification Breakdown (rows = true class):\n",
      "        Class 0 misclassified as: 27.8% → class 1, 51.4% → class 2\n",
      "        Class 1 misclassified as: 13.9% → class 0, 34.7% → class 2\n",
      "        Class 2 misclassified as: 9.7% → class 0, 36.1% → class 1\n",
      "\n",
      "- Domain 11:\n",
      "    - Accuracy: 0.7315\n",
      "    - Class 0: F1 Score = 0.5517\n",
      "    - Class 1: F1 Score = 0.8489\n",
      "    - Class 2: F1 Score = 0.7571\n",
      "    - Misclassification Breakdown (rows = true class):\n",
      "        Class 0 misclassified as: 11.1% → class 1, 44.4% → class 2\n",
      "        Class 1 misclassified as: 9.7% → class 0, 8.3% → class 2\n",
      "        Class 2 misclassified as: 6.9% → class 0, 0.0% → class 1\n",
      "\n",
      "- Domain 12:\n",
      "    - Accuracy: 0.6435\n",
      "    - Class 0: F1 Score = 0.6040\n",
      "    - Class 1: F1 Score = 0.7907\n",
      "    - Class 2: F1 Score = 0.5584\n",
      "    - Misclassification Breakdown (rows = true class):\n",
      "        Class 0 misclassified as: 5.6% → class 1, 31.9% → class 2\n",
      "        Class 1 misclassified as: 6.9% → class 0, 22.2% → class 2\n",
      "        Class 2 misclassified as: 37.5% → class 0, 2.8% → class 1\n",
      "\n",
      "- Domain 13:\n",
      "    - Accuracy: 0.5714\n",
      "    - Class 0: F1 Score = 0.7541\n",
      "    - Class 1: F1 Score = 0.4762\n",
      "    - Class 2: F1 Score = 0.5000\n",
      "    - Misclassification Breakdown (rows = true class):\n",
      "        Class 0 misclassified as: 14.7% → class 1, 17.6% → class 2\n",
      "        Class 1 misclassified as: 2.9% → class 0, 54.3% → class 2\n",
      "        Class 2 misclassified as: 10.3% → class 0, 27.6% → class 1\n",
      "\n",
      "- Domain 14:\n",
      "    - Accuracy: 0.4353\n",
      "    - Class 0: F1 Score = 0.3929\n",
      "    - Class 1: F1 Score = 0.4000\n",
      "    - Class 2: F1 Score = 0.5085\n",
      "    - Misclassification Breakdown (rows = true class):\n",
      "        Class 0 misclassified as: 38.7% → class 1, 25.8% → class 2\n",
      "        Class 1 misclassified as: 24.1% → class 0, 37.9% → class 2\n",
      "        Class 2 misclassified as: 28.0% → class 0, 12.0% → class 1\n",
      "\n",
      "- Domain 15:\n",
      "    - Accuracy: 0.4255\n",
      "    - Class 0: F1 Score = 0.4800\n",
      "    - Class 1: F1 Score = 0.4068\n",
      "    - Class 2: F1 Score = 0.4051\n",
      "    - Misclassification Breakdown (rows = true class):\n",
      "        Class 0 misclassified as: 16.7% → class 1, 50.0% → class 2\n",
      "        Class 1 misclassified as: 3.7% → class 0, 51.9% → class 2\n",
      "        Class 2 misclassified as: 3.2% → class 0, 45.2% → class 1\n",
      "\n",
      "- Domain 16:\n",
      "    - Accuracy: 0.4141\n",
      "    - Class 0: F1 Score = 0.3390\n",
      "    - Class 1: F1 Score = 0.3860\n",
      "    - Class 2: F1 Score = 0.4878\n",
      "    - Misclassification Breakdown (rows = true class):\n",
      "        Class 0 misclassified as: 19.4% → class 1, 48.4% → class 2\n",
      "        Class 1 misclassified as: 28.6% → class 0, 40.0% → class 2\n",
      "        Class 2 misclassified as: 24.2% → class 0, 15.2% → class 1\n",
      "\n",
      "- Domain 17:\n",
      "    - Accuracy: 0.7174\n",
      "    - Class 0: F1 Score = 0.7541\n",
      "    - Class 1: F1 Score = 0.7407\n",
      "    - Class 2: F1 Score = 0.6667\n",
      "    - Misclassification Breakdown (rows = true class):\n",
      "        Class 0 misclassified as: 3.6% → class 1, 14.3% → class 2\n",
      "        Class 1 misclassified as: 6.5% → class 0, 29.0% → class 2\n",
      "        Class 2 misclassified as: 24.2% → class 0, 6.1% → class 1\n",
      "\n",
      "- Domain 18:\n",
      "    - Accuracy: 0.6747\n",
      "    - Class 0: F1 Score = 0.6364\n",
      "    - Class 1: F1 Score = 0.7353\n",
      "    - Class 2: F1 Score = 0.6296\n",
      "    - Misclassification Breakdown (rows = true class):\n",
      "        Class 0 misclassified as: 28.6% → class 1, 4.8% → class 2\n",
      "        Class 1 misclassified as: 3.4% → class 0, 10.3% → class 2\n",
      "        Class 2 misclassified as: 24.2% → class 0, 24.2% → class 1\n",
      "\n",
      "- Domain 19:\n",
      "    - Accuracy: 0.7241\n",
      "    - Class 0: F1 Score = 0.6182\n",
      "    - Class 1: F1 Score = 0.6923\n",
      "    - Class 2: F1 Score = 0.8358\n",
      "    - Misclassification Breakdown (rows = true class):\n",
      "        Class 0 misclassified as: 15.4% → class 1, 19.2% → class 2\n",
      "        Class 1 misclassified as: 30.0% → class 0, 10.0% → class 2\n",
      "        Class 2 misclassified as: 9.7% → class 0, 0.0% → class 1\n",
      "\n",
      "- Domain 20:\n",
      "    - Accuracy: 0.8152\n",
      "    - Class 0: F1 Score = 0.7407\n",
      "    - Class 1: F1 Score = 0.7797\n",
      "    - Class 2: F1 Score = 0.9014\n",
      "    - Misclassification Breakdown (rows = true class):\n",
      "        Class 0 misclassified as: 17.9% → class 1, 10.7% → class 2\n",
      "        Class 1 misclassified as: 16.7% → class 0, 6.7% → class 2\n",
      "        Class 2 misclassified as: 2.9% → class 0, 2.9% → class 1\n",
      "\n",
      "- Domain 21:\n",
      "    - Accuracy: 0.7979\n",
      "    - Class 0: F1 Score = 0.8846\n",
      "    - Class 1: F1 Score = 0.7143\n",
      "    - Class 2: F1 Score = 0.8000\n",
      "    - Misclassification Breakdown (rows = true class):\n",
      "        Class 0 misclassified as: 7.1% → class 1, 10.7% → class 2\n",
      "        Class 1 misclassified as: 3.1% → class 0, 34.4% → class 2\n",
      "        Class 2 misclassified as: 0.0% → class 0, 5.9% → class 1\n",
      "\n",
      "- Domain 22:\n",
      "    - Accuracy: 0.3846\n",
      "    - Class 0: F1 Score = 0.2326\n",
      "    - Class 1: F1 Score = 0.4375\n",
      "    - Class 2: F1 Score = 0.4267\n",
      "    - Misclassification Breakdown (rows = true class):\n",
      "        Class 0 misclassified as: 32.4% → class 1, 54.1% → class 2\n",
      "        Class 1 misclassified as: 3.7% → class 0, 44.4% → class 2\n",
      "        Class 2 misclassified as: 0.0% → class 0, 40.7% → class 1\n",
      "\n",
      "Overall Metrics (All Domains):\n",
      "    - Overall Accuracy: 0.6421\n",
      "    - Class 0: F1 Score = 0.6394\n",
      "    - Class 1: F1 Score = 0.6353\n",
      "    - Class 2: F1 Score = 0.6503\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "data = torch.load('./Dataset/eeg_data_200.pt', map_location=device)\n",
    "\n",
    "X_tensor = data['X']\n",
    "YD_tensor = data['YD']\n",
    "padding_masks_tensor = data['mask']\n",
    "channel_xy_tensor = data['channel_xy']\n",
    "\n",
    "X_tensor_half = data['X_val']\n",
    "YD_tensor_half = data['YD_val']\n",
    "padding_masks_tensor_half = data['mask_val']\n",
    "\n",
    "X_tensor_test = data['X_test']\n",
    "YD_tensor_test = data['YD_test']\n",
    "padding_masks_tensor_test = data['mask_test']\n",
    "\n",
    "dataset = MultisourceDataset(X_tensor, YD_tensor, padding_masks_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=10, shuffle=True)\n",
    "dataset_test = MultisourceDataset(X_tensor_half, YD_tensor_half, padding_masks_tensor_half)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=10, shuffle=True)\n",
    "dataset_test0 = MultisourceDataset(X_tensor_test, YD_tensor_test, padding_masks_tensor_test)\n",
    "dataloader_test0 = DataLoader(dataset_test0, batch_size=10, shuffle=True)\n",
    "\n",
    "# === Seeding ===\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# === Hyperparameters ===\n",
    "Chans = 60  # Input EEG channels\n",
    "Samples = 841  # Number of time samples\n",
    "num_classes = 3\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# === Test Loader ===\n",
    "test_loader = DataLoader(dataloader_test0.dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# === Initialize and Load One Model ===\n",
    "model = EEGNet0(nb_classes=num_classes, Chans=Chans, Samples=Samples).to(device)\n",
    "model.eval()\n",
    "\n",
    "# === Load Checkpoint ===\n",
    "save_path = \"./BaselineModel/EEGNet0-X.pth\"\n",
    "if not os.path.exists(save_path):\n",
    "    raise FileNotFoundError(f\"Model checkpoint not found: {save_path}\")\n",
    "\n",
    "checkpoint = torch.load(save_path)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# === Containers for Evaluation ===\n",
    "overall_preds = []\n",
    "overall_targets = []\n",
    "domain_wise_results = {}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        x_batch, yd_batch, mask_batch = batch\n",
    "        domains = yd_batch[:, 4].cpu().numpy()\n",
    "        targets = yd_batch[:, 0].cpu().numpy()\n",
    "\n",
    "        # Apply masking and reshape: (B, 1, Chans, Samples)\n",
    "        # x_valid = x_batch[mask_batch].reshape(x_batch.shape[0], -1, x_batch.shape[-1])\n",
    "        x_valid = x_batch\n",
    "        x_valid = x_valid.unsqueeze(1).to(device)\n",
    "\n",
    "        logits, _ = model(x_valid)\n",
    "        predictions = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "\n",
    "        # Store all predictions and targets\n",
    "        overall_preds.extend(predictions)\n",
    "        overall_targets.extend(targets)\n",
    "\n",
    "        # Store per-domain\n",
    "        for i in range(len(domains)):\n",
    "            dom = int(domains[i])\n",
    "            if dom not in domain_wise_results:\n",
    "                domain_wise_results[dom] = {\"y_true\": [], \"y_pred\": []}\n",
    "            domain_wise_results[dom][\"y_true\"].append(targets[i])\n",
    "            domain_wise_results[dom][\"y_pred\"].append(predictions[i])\n",
    "\n",
    "# === Per-Domain Metrics ===\n",
    "print(\"Metrics by Domain (Using One Model):\")\n",
    "for dom in sorted(domain_wise_results.keys()):\n",
    "    y_true = domain_wise_results[dom][\"y_true\"]\n",
    "    y_pred = domain_wise_results[dom][\"y_pred\"]\n",
    "\n",
    "    print(f\"\\n- Domain {dom}:\")\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    print(f\"    - Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    classes = sorted(set(y_true))\n",
    "    f1_scores = f1_score(y_true, y_pred, average=None, labels=classes)\n",
    "    for cls, f1 in zip(classes, f1_scores):\n",
    "        print(f\"    - Class {cls}: F1 Score = {f1:.4f}\")\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=classes)\n",
    "    cm_percent = cm.astype(float) / cm.sum(axis=1, keepdims=True)\n",
    "\n",
    "    print(f\"    - Misclassification Breakdown (rows = true class):\")\n",
    "    for i, cls in enumerate(classes):\n",
    "        breakdown = []\n",
    "        for j, pred_cls in enumerate(classes):\n",
    "            if i == j:\n",
    "                continue\n",
    "            pct = cm_percent[i, j] * 100\n",
    "            breakdown.append(f\"{pct:.1f}% → class {pred_cls}\")\n",
    "        if breakdown:\n",
    "            print(f\"        Class {cls} misclassified as: {', '.join(breakdown)}\")\n",
    "        else:\n",
    "            print(f\"        Class {cls} has no misclassifications.\")\n",
    "\n",
    "# === Overall Metrics ===\n",
    "if overall_preds:\n",
    "    overall_accuracy = accuracy_score(overall_targets, overall_preds)\n",
    "    all_classes = sorted(set(overall_targets))\n",
    "    f1_per_class = f1_score(overall_targets, overall_preds, average=None, labels=all_classes)\n",
    "\n",
    "    print(\"\\nOverall Metrics (All Domains):\")\n",
    "    print(f\"    - Overall Accuracy: {overall_accuracy:.4f}\")\n",
    "    for cls, f1 in zip(all_classes, f1_per_class):\n",
    "        print(f\"    - Class {cls}: F1 Score = {f1:.4f}\")\n",
    "else:\n",
    "    print(\"\\nNo predictions were collected. Please check masking or data loading.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
