{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34770d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from mne.decoding import CSP\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "import moabb\n",
    "from moabb.datasets import BNCI2014_001, Zhou2016, Schirrmeister2017, Weibo2014\n",
    "from moabb.evaluations import WithinSessionEvaluation\n",
    "from moabb.paradigms import LeftRightImagery\n",
    "\n",
    "moabb.set_log_level(\"info\")\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# moabb.set_download_dir(\"D:\\TA\\database\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cf97fd",
   "metadata": {},
   "source": [
    "DATA LOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3d803aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class MultisourceDataset(Dataset):\n",
    "    def __init__(self, X, YD, channel_mask):\n",
    "        self.X = X\n",
    "        self.YD = YD\n",
    "        self.channel_mask = channel_mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx]\n",
    "        yd = self.YD[idx]\n",
    "        masks = self.channel_mask[idx]\n",
    "        return x, yd, masks  # Return data + index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16140f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Sampler\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "class MultiDomainBatchSampler(Sampler):\n",
    "    def __init__(self, yd, batch_size):\n",
    "        self.batch_size = batch_size\n",
    "        self.domain_to_indices = defaultdict(list)\n",
    "\n",
    "        # Collect indices for each domain\n",
    "        for idx, label in enumerate(yd):\n",
    "            domain = int(label[4])  # domain info is in 5th column\n",
    "            self.domain_to_indices[domain].append(idx)\n",
    "\n",
    "        self.domains = list(self.domain_to_indices.keys())\n",
    "\n",
    "    def __iter__(self):\n",
    "        # Create a shuffled iterator for each domain\n",
    "        domain_iters = {\n",
    "            d: iter(random.sample(v, len(v)))\n",
    "            for d, v in self.domain_to_indices.items()\n",
    "        }\n",
    "        domain_cycle = self.domains.copy()\n",
    "        random.shuffle(domain_cycle)\n",
    "\n",
    "        # Track active domains\n",
    "        active_domains = set(domain_iters.keys())\n",
    "        batch = []\n",
    "\n",
    "        while active_domains:\n",
    "            for domain in domain_cycle:\n",
    "                if domain not in active_domains:\n",
    "                    # If domain is exhausted, pick a random sample from the original pool\n",
    "                    idx = random.choice(self.domain_to_indices[domain])\n",
    "                else:\n",
    "                    try:\n",
    "                        # Pick next in the shuffled list\n",
    "                        idx = next(domain_iters[domain])\n",
    "                    except StopIteration:\n",
    "                        # If exhausted, switch to random sampling\n",
    "                        active_domains.remove(domain)\n",
    "                        idx = random.choice(self.domain_to_indices[domain])\n",
    "\n",
    "                batch.append(idx)\n",
    "\n",
    "                if len(batch) == self.batch_size:\n",
    "                    yield batch\n",
    "                    batch = []\n",
    "\n",
    "        if batch:\n",
    "            yield batch\n",
    "\n",
    "    def __len__(self):\n",
    "        return sum(len(v) for v in self.domain_to_indices.values()) // self.batch_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1257a400",
   "metadata": {},
   "source": [
    "eeg_data_200 = multi-source dataset\n",
    "eeg_data_200_cross = multi-source dataset for cross subject classification\n",
    "eeg_data_bnci = BNCI2014_001 subset of multi-source dataset\n",
    "eeg_data_zhou = Zhou2016 subset of multi-source dataset\n",
    "eeg_data_weibo = Weibo2014 subset of multi-source dataset\n",
    "\n",
    "X = training rows/trials\n",
    "YD = training labels\n",
    "mask = training channel masks\n",
    "\n",
    "X_val = validatiom rows/trials\n",
    "YD_val = validation labels\n",
    "mask_val = validation channel masks\n",
    "\n",
    "X_test = test rows/trials\n",
    "YD_test = test labels\n",
    "mask_test = test channel masks\n",
    "\n",
    "channel_xy = list of channels positions\n",
    "\n",
    "Labels = [class, dataset sources id, subject id (of each dataset), session id (of each subject), domain id]\n",
    "domain id is unique\n",
    "combination (dataset sources id, subject id) is unique\n",
    "combination (dataset sources id, subject id, session id) is unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0720431f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "data = torch.load('./Dataset/eeg_data_200.pt', map_location=device)\n",
    "\n",
    "X_tensor = data['X']\n",
    "YD_tensor = data['YD']\n",
    "padding_masks_tensor = data['mask']\n",
    "channel_xy_tensor = data['channel_xy']\n",
    "\n",
    "X_tensor_half = data['X_val']\n",
    "YD_tensor_half = data['YD_val']\n",
    "padding_masks_tensor_half = data['mask_val']\n",
    "\n",
    "X_tensor_test = data['X_test']\n",
    "YD_tensor_test = data['YD_test']\n",
    "padding_masks_tensor_test = data['mask_test']\n",
    "\n",
    "dataset = MultisourceDataset(X_tensor, YD_tensor, padding_masks_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=10, shuffle=True)\n",
    "dataset_test = MultisourceDataset(X_tensor_half, YD_tensor_half, padding_masks_tensor_half)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=10, shuffle=True)\n",
    "dataset_test0 = MultisourceDataset(X_tensor_test, YD_tensor_test, padding_masks_tensor_test)\n",
    "dataloader_test0 = DataLoader(dataset_test0, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2749ed7",
   "metadata": {},
   "source": [
    "SPATIAL ATTENTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ead5a894",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def compute_cos_sin(x, y, K):\n",
    "    kk = torch.arange(1, K+1, device=device)\n",
    "    ll = torch.arange(1, K+1, device=device)\n",
    "    cos_fun = lambda k, l, x, y: torch.cos(2*torch.pi*(k*x + l*y))\n",
    "    sin_fun = lambda k, l, x, y: torch.sin(2*torch.pi*(k*x + l*y))\n",
    "    return torch.stack([cos_fun(kk[None,:], ll[:,None], x, y) for x, y in zip(x, y)]).reshape(x.shape[0],-1).float(), torch.stack([sin_fun(kk[None,:], ll[:,None], x, y) for x, y in zip(x, y)]).reshape(x.shape[0],-1).float()\n",
    "\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, out_channels, K, dropout=False, seed=None):\n",
    "        super().__init__()\n",
    "        self.outchans = out_channels\n",
    "        self.K = K\n",
    "        self.z = nn.Parameter(\n",
    "            torch.randn(self.outchans, K * K, dtype=torch.cfloat, device=device) / (K * K)\n",
    "        )  # Each output channel has its own KxK z matrix\n",
    "        self.z.requires_grad = True\n",
    "        self.dropout = dropout\n",
    "        self.seed = seed\n",
    "\n",
    "    def forward(self, X, masks, channel_xy):\n",
    "        batch_size, _, _ = X.shape  # (batch, 27, T)\n",
    "\n",
    "        # Move everything to device before processing\n",
    "        X = X.to(device)\n",
    "        masks = masks.to(device)\n",
    "        channel_xy = channel_xy.to(device)\n",
    "\n",
    "        # Masking: Extract only valid channels\n",
    "        X_valid = [X[i][masks[i]] for i in range(batch_size)]  # List of tensors with different shapes\n",
    "        ch_pos_valid = [channel_xy[masks[i]] for i in range(batch_size)]  # Different per sample\n",
    "\n",
    "        # Compute cosine and sine matrices\n",
    "        cos_sin_matrices = [compute_cos_sin(ch[:, 0], ch[:, 1], self.K) for ch in ch_pos_valid]\n",
    "\n",
    "        attended_outputs = []\n",
    "        for i in range(batch_size):\n",
    "            cos_mat, sin_mat = cos_sin_matrices[i]\n",
    "            cos_mat, sin_mat = cos_mat.to(device), sin_mat.to(device)\n",
    "\n",
    "            # Compute attention weights\n",
    "            a = torch.matmul(self.z.real, cos_mat.T) + torch.matmul(self.z.imag, sin_mat.T)\n",
    "\n",
    "            if self.dropout and X_valid[i].shape[0] > 1:\n",
    "                # print(ch_pos_valid.shape)\n",
    "                # Get min-max x and y positions\n",
    "                min_x, max_x = ch_pos_valid[i][:, 0].min(), ch_pos_valid[i][:, 0].max()\n",
    "                min_y, max_y = ch_pos_valid[i][:, 1].min(), ch_pos_valid[i][:, 1].max()\n",
    "\n",
    "                # Sample a random position within this range\n",
    "                if self.seed is not None:\n",
    "                    gen = torch.Generator(device=device).manual_seed(self.seed)\n",
    "                    rand_x = torch.empty(1, device=device).uniform_(min_x, max_x, generator=gen).item()\n",
    "                    rand_y = torch.empty(1, device=device).uniform_(min_y, max_y, generator=gen).item()\n",
    "                else:\n",
    "                    rand_x = torch.FloatTensor(1).uniform_(min_x, max_x).item()\n",
    "                    rand_y = torch.FloatTensor(1).uniform_(min_y, max_y).item()\n",
    "\n",
    "                # Compute Euclidean distance from the random point to all channels\n",
    "                distances = (ch_pos_valid[i][:, 0] - rand_x) ** 2 + (ch_pos_valid[i][:, 1] - rand_y) ** 2\n",
    "\n",
    "                # Find indices of channels within a 000.1 distance radius\n",
    "                drop_mask = distances < 0.001\n",
    "\n",
    "                # Drop the selected channels\n",
    "                X_valid[i] = X_valid[i][~drop_mask]  # Keep only non-dropped channels\n",
    "                a = a[:, ~drop_mask]  # Adjust attention weights accordingly\n",
    "\n",
    "            a = F.softmax(a, dim=1)  # Normalize over all valid input channels\n",
    "\n",
    "            # Apply attention to EEG data\n",
    "            attended = torch.matmul(a, X_valid[i])  # (out_channels, T)\n",
    "            attended_outputs.append(attended)\n",
    "\n",
    "        # Pad to ensure uniform shape\n",
    "        max_len = max(att.shape[1] for att in attended_outputs)\n",
    "        padded_attended = torch.stack([F.pad(att, (0, max_len - att.shape[1])) for att in attended_outputs])\n",
    "\n",
    "        return padded_attended\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6d2511",
   "metadata": {},
   "source": [
    "EEG_DG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "810d97c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "class DG_Network(nn.Module):\n",
    "    def __init__(self, classes, domains, feature_size=4096, F1=4, D=2, channels=14):\n",
    "        super(DG_Network, self).__init__()\n",
    "        self.dropout = 0.25  # default:0.25\n",
    "\n",
    "        self.special_features = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(feature_size, 400),\n",
    "                nn.Dropout(self.dropout)\n",
    "            )\n",
    "            for _ in range(domains)\n",
    "        ])\n",
    "\n",
    "        # self.bn = nn.BatchNorm1d(400)\n",
    "\n",
    "        self.domain_classifier = nn.Sequential(\n",
    "            nn.Linear(feature_size, domains),\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(400, classes)\n",
    "        )\n",
    "\n",
    "        self.block1_1 = nn.Sequential(\n",
    "            nn.ZeroPad2d((3, 4, 0, 0)),\n",
    "            nn.Conv2d(1, F1, kernel_size=(1, 8), bias=False),\n",
    "            nn.BatchNorm2d(F1)\n",
    "        )\n",
    "\n",
    "        self.block1_2 = nn.Sequential(\n",
    "            nn.ZeroPad2d((7, 8, 0, 0)),\n",
    "            nn.Conv2d(1, F1, kernel_size=(1, 16), bias=False),\n",
    "            nn.BatchNorm2d(F1)\n",
    "        )\n",
    "\n",
    "        self.block1_3 = nn.Sequential(\n",
    "            nn.ZeroPad2d((15, 16, 0, 0)),\n",
    "            nn.Conv2d(1, F1, kernel_size=(1, 32), bias=False),\n",
    "            nn.BatchNorm2d(F1)\n",
    "        )\n",
    "\n",
    "        self.block1_4 = nn.Sequential(\n",
    "            nn.ZeroPad2d((31, 32, 0, 0)),\n",
    "            nn.Conv2d(1, F1, kernel_size=(1, 64), bias=False),\n",
    "            nn.BatchNorm2d(F1)\n",
    "        )\n",
    "\n",
    "        self.block2 = nn.Sequential(\n",
    "            # DepthwiseConv2D\n",
    "            nn.Conv2d(F1 * 4, F1 * 4 * D, kernel_size=(channels, 1), groups=F1 * 4, bias=False),\n",
    "            # groups=F1 for depthWiseConv\n",
    "            nn.BatchNorm2d(F1 * 4 * D),\n",
    "            # nn.ELU(inplace=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AvgPool2d((1, 4)),\n",
    "            nn.Dropout(self.dropout),\n",
    "        )\n",
    "\n",
    "        self.block3_1 = nn.Sequential(\n",
    "            # SeparableConv2D\n",
    "            nn.ZeroPad2d((0, 1, 0, 0)),\n",
    "            nn.Conv2d(F1 * 4 * D, F1 * 4 * D, kernel_size=(1, 2), groups=F1 * 4 * D, bias=False),\n",
    "            # groups=F1 for depthWiseConv\n",
    "            nn.BatchNorm2d(F1 * 4 * D),\n",
    "            # nn.ELU(inplace=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(F1 * 4 * D, F1 * 4 * D, kernel_size=(1, 1), groups=1, bias=False),  # point-wise cnn\n",
    "            nn.BatchNorm2d(F1 * 4 * D),\n",
    "        )\n",
    "\n",
    "        self.block3_2 = nn.Sequential(\n",
    "            # SeparableConv2D\n",
    "            nn.ZeroPad2d((1, 2, 0, 0)),\n",
    "            nn.Conv2d(F1 * 4 * D, F1 * 4 * D, kernel_size=(1, 4), groups=F1 * 4 * D, bias=False),\n",
    "            # groups=F1 for depthWiseConv\n",
    "            nn.BatchNorm2d(F1 * 4 * D),\n",
    "            # nn.ELU(inplace=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(F1 * 4 * D, F1 * 4 * D, kernel_size=(1, 1), groups=1, bias=False),  # point-wise cnn\n",
    "            nn.BatchNorm2d(F1 * 4 * D),\n",
    "        )\n",
    "\n",
    "        self.block3_3 = nn.Sequential(\n",
    "            # SeparableConv2D\n",
    "            nn.ZeroPad2d((3, 4, 0, 0)),\n",
    "            nn.Conv2d(F1 * 4 * D, F1 * 4 * D, kernel_size=(1, 8), groups=F1 * 4 * D, bias=False),\n",
    "            # groups=F1 for depthWiseConv\n",
    "            nn.BatchNorm2d(F1 * 4 * D),\n",
    "            # nn.ELU(inplace=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(F1 * 4 * D, F1 * 4 * D, kernel_size=(1, 1), groups=1, bias=False),  # point-wise cnn\n",
    "            nn.BatchNorm2d(F1 * 4 * D),\n",
    "        )\n",
    "\n",
    "        self.block3_4 = nn.Sequential(\n",
    "            # SeparableConv2D\n",
    "            nn.ZeroPad2d((7, 8, 0, 0)),\n",
    "            nn.Conv2d(F1 * 4 * D, F1 * 4 * D, kernel_size=(1, 16), groups=F1 * 4 * D, bias=False),\n",
    "            # groups=F1 for depthWiseConv\n",
    "            nn.BatchNorm2d(F1 * 4 * D),\n",
    "            # nn.ELU(inplace=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(F1 * 4 * D, F1 * 4 * D, kernel_size=(1, 1), groups=1, bias=False),  # point-wise cnn\n",
    "            nn.BatchNorm2d(F1 * 4 * D),\n",
    "        )\n",
    "\n",
    "        self.block4 = nn.Sequential(\n",
    "            # nn.ELU(inplace=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AvgPool2d((1, 8)),\n",
    "            nn.Dropout(self.dropout)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, features):\n",
    "\n",
    "        feat_1 = self.block1_1(features)\n",
    "        feat_2 = self.block1_2(features)\n",
    "        feat_3 = self.block1_3(features)\n",
    "        feat_4 = self.block1_4(features)\n",
    "        feat = torch.cat((feat_1, feat_2, feat_3, feat_4), dim=1)\n",
    "\n",
    "        feature = self.block2(feat)\n",
    "\n",
    "        feature_1 = self.block3_1(feature)\n",
    "        feature_2 = self.block3_2(feature)\n",
    "        feature_3 = self.block3_3(feature)\n",
    "        feature_4 = self.block3_4(feature)\n",
    "        features = torch.cat((feature_1, feature_2, feature_3, feature_4), dim=1)\n",
    "\n",
    "        features = self.block4(features)\n",
    "\n",
    "        features = torch.flatten(features, 1)\n",
    "\n",
    "        # Assuming `features` is the input tensor of shape [batch_size, feature_size]\n",
    "        Feat_s = [special(features) for special in self.special_features]\n",
    "\n",
    "        # feat for domain classifier, dom for computing domain specific loss\n",
    "        feat_ = self.domain_classifier(features)\n",
    "        weight = nn.functional.softmax(feat_, dim=1)\n",
    "\n",
    "        featAll = torch.stack(Feat_s, dim=1)\n",
    "        weighted = weight.unsqueeze(0).permute(1, 0, 2)\n",
    "        weighted_feature = torch.bmm(weighted, featAll)\n",
    "        weighted_feature = torch.flatten(weighted_feature, 1)\n",
    "        # weighted_feature = self.bn(weighted_feature)\n",
    "        logits = self.classifier(weighted_feature)\n",
    "\n",
    "        if self.training:\n",
    "            return logits, feat_, Feat_s, weighted_feature\n",
    "        else:\n",
    "            return logits, weighted_feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "201040a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "class Dist_Loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Dist_Loss, self).__init__()\n",
    "\n",
    "\n",
    "    def intraclass_compactness(self, data, labels):\n",
    "        unique_labels = torch.unique(labels)\n",
    "        compactness = torch.tensor(0.0, device=data.device)\n",
    "\n",
    "        for label in unique_labels:\n",
    "            class_data = data[labels == label]\n",
    "\n",
    "            distances = torch.cdist(class_data, class_data, p=2)\n",
    "            compactness += torch.sum(distances) / 2\n",
    "\n",
    "        return compactness / data.shape[0]\n",
    "\n",
    "\n",
    "    def interclass_separability(self, data, labels):\n",
    "        unique_labels = torch.unique(labels)\n",
    "        separability = torch.tensor(0.0, device=data.device)\n",
    "\n",
    "        for i in range(len(unique_labels)):\n",
    "            for j in range(len(unique_labels)):\n",
    "                if i != j:\n",
    "                  class_data_1 = data[labels == unique_labels[i]]\n",
    "                  class_data_2 = data[labels == unique_labels[j]]\n",
    "\n",
    "                  distances = torch.cdist(class_data_1, class_data_2, p=2)\n",
    "                  separability += torch.sum(distances)\n",
    "\n",
    "        return separability / data.shape[0]\n",
    "\n",
    "    def compute_class_centers(self, data, labels, all_unique_labels):\n",
    "        class_centers = torch.zeros(len(all_unique_labels), data.size(1), device=data.device)\n",
    "        valid_mask = torch.zeros(len(all_unique_labels), dtype=torch.bool, device=data.device)\n",
    "\n",
    "        for label in torch.unique(labels):\n",
    "            class_data = data[labels == label]\n",
    "            index = (all_unique_labels == label).nonzero(as_tuple=True)[0].item()\n",
    "\n",
    "            if class_data.size(0) > 0:\n",
    "                class_center = torch.mean(class_data, dim=0)\n",
    "                class_centers[index] = class_center\n",
    "                valid_mask[index] = True\n",
    "\n",
    "        return class_centers, valid_mask\n",
    "\n",
    "\n",
    "    def forward(self, all_data, all_labels, alpha):\n",
    "        num_domains = len(all_data)\n",
    "        total_dist = torch.tensor(0.0, device=all_data[0].device)\n",
    "\n",
    "        # Step 1: Intra + Inter losses for each domain\n",
    "        for data, labels in zip(all_data, all_labels):\n",
    "            dist_1 = self.intraclass_compactness(data, labels)\n",
    "            # print(dist_1.item())\n",
    "            dist_2 = self.interclass_separability(data, labels)\n",
    "            # print(dist_2.item())\n",
    "            total_dist += dist_1 - alpha * dist_2\n",
    "\n",
    "        all_unique_labels = torch.unique(torch.cat(all_labels))\n",
    "\n",
    "        all_centers = []\n",
    "        all_valid_masks = []\n",
    "\n",
    "        for data, labels in zip(all_data, all_labels):\n",
    "            centers, valid_mask = self.compute_class_centers(data, labels, all_unique_labels)\n",
    "            all_centers.append(centers)\n",
    "            all_valid_masks.append(valid_mask)\n",
    "\n",
    "\n",
    "        # Step 3: Compute pairwise dist_31 across different domains only\n",
    "        dist_31 = torch.tensor(0.0, device=all_data[0].device)\n",
    "\n",
    "        for i in range(num_domains):\n",
    "            for j in range(i, num_domains):\n",
    "                src_centers, src_valid = all_centers[i], all_valid_masks[i]\n",
    "                tgt_centers, tgt_valid = all_centers[j], all_valid_masks[j]\n",
    "\n",
    "                valid_indices = [k for k in range(len(src_centers)) if src_valid[k] and tgt_valid[k]]\n",
    "\n",
    "                if valid_indices:\n",
    "                    valid_idx_tensor = torch.tensor(valid_indices, device=src_centers.device)\n",
    "                    src = src_centers[valid_idx_tensor]\n",
    "                    tgt = tgt_centers[valid_idx_tensor]\n",
    "\n",
    "                    matrix = torch.cdist(src, tgt, p=2)\n",
    "                    diagonal = torch.diag(matrix)\n",
    "                    dist_31 += torch.mean(diagonal)\n",
    "                    # print(dist_31.item())\n",
    "\n",
    "        return total_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0394b19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class MMD_loss(nn.Module):\n",
    "    def __init__(self, kernel_type='rbf', kernel_mul=2.0, kernel_num=5, eps=1e-8):\n",
    "        super(MMD_loss, self).__init__()\n",
    "        self.kernel_num = kernel_num\n",
    "        self.kernel_mul = kernel_mul\n",
    "        self.fix_sigma = None\n",
    "        self.kernel_type = kernel_type\n",
    "        self.eps = eps\n",
    "\n",
    "    def guassian_kernel(self, source, target, kernel_mul=2.0, kernel_num=5, fix_sigma=None):\n",
    "        n_samples = int(source.size()[0]) + int(target.size()[0])\n",
    "        total = torch.cat([source, target], dim=0)\n",
    "        total0 = total.unsqueeze(0).expand(int(total.size(0)), int(total.size(0)), int(total.size(1)))\n",
    "        total1 = total.unsqueeze(1).expand(int(total.size(0)), int(total.size(0)), int(total.size(1)))\n",
    "        L2_distance = ((total0-total1)**2).sum(2)\n",
    "        if fix_sigma:\n",
    "            bandwidth = fix_sigma\n",
    "        else:\n",
    "            bandwidth = torch.sum(L2_distance.data) / (n_samples ** 2 - n_samples + self.eps)\n",
    "        bandwidth /= kernel_mul ** (kernel_num // 2)\n",
    "        bandwidth_list = [bandwidth * (kernel_mul**i) for i in range(kernel_num)]\n",
    "        kernel_val = [torch.exp(-L2_distance / (bandwidth_temp + self.eps)) for bandwidth_temp in bandwidth_list]\n",
    "        return sum(kernel_val)\n",
    "\n",
    "    def linear_mmd2(self, f_of_X, f_of_Y):\n",
    "        delta = f_of_X.float().mean(0) - f_of_Y.float().mean(0)\n",
    "        loss = delta.dot(delta.T)\n",
    "        return loss\n",
    "\n",
    "    def forward(self, source_all):\n",
    "        # Compute mean across domains → shape: [batch_size, feat_dim]\n",
    "        target = torch.mean(source_all, dim=0)\n",
    "\n",
    "        total_loss = torch.tensor(0.0, device=source_all.device)\n",
    "        for i in range(source_all.shape[0]):\n",
    "            source = source_all[i]\n",
    "            if self.kernel_type == 'linear':\n",
    "                total_loss += self.linear_mmd2(source, target)\n",
    "            elif self.kernel_type == 'rbf':\n",
    "                batch_size = int(source.shape[0])\n",
    "                kernels = self.guassian_kernel(\n",
    "                    source, target,\n",
    "                    kernel_mul=self.kernel_mul,\n",
    "                    kernel_num=self.kernel_num,\n",
    "                    fix_sigma=self.fix_sigma\n",
    "                )\n",
    "                XX = torch.mean(kernels[:batch_size, :batch_size])\n",
    "                YY = torch.mean(kernels[batch_size:, batch_size:])\n",
    "                XY = torch.mean(kernels[:batch_size, batch_size:])\n",
    "                YX = torch.mean(kernels[batch_size:, :batch_size])\n",
    "                loss = torch.mean(XX + YY - XY - YX)\n",
    "                total_loss += loss\n",
    "        return total_loss / source_all.shape[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6710ecdc",
   "metadata": {},
   "source": [
    "TRAIN LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a5a2b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⛔ Training interrupted. Saving model...\n",
      "✅ Model saved. Exiting cleanly.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "# Set hyperparameters\n",
    "out_channels = 46\n",
    "K = 48\n",
    "num_classes = 3\n",
    "learning_rate = 0.0001\n",
    "num_epochs = 500 #166\n",
    "train_ratio = 0.7  # 70% train, 30% validation\n",
    "\n",
    "# Initialize model\n",
    "model = SpatialAttention(out_channels, K, dropout=True, seed=seed).to(device)\n",
    "pool = nn.AvgPool1d(kernel_size=75, stride=15)\n",
    "\n",
    "# Classification head\n",
    "num_domains = 23\n",
    "dg = DG_Network(num_classes, num_domains, channels=out_channels, feature_size=3328).to(device)\n",
    "dl = Dist_Loss().to(device)\n",
    "mmdl = MMD_loss().to(device)\n",
    "softmax = nn.Softmax(dim=1)\n",
    "\n",
    "dist_weight = 0.1\n",
    "mmd_weigth = 0.1\n",
    "dom_weight = 0.15\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "criterion_dom = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "optimizer = optim.Adam(\n",
    "    list(model.parameters()) +\n",
    "    list(dg.parameters()), lr=learning_rate, weight_decay=5e-4\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "sampler = MultiDomainBatchSampler(dataloader.dataset.YD, batch_size=30)\n",
    "train_loader = DataLoader(dataloader.dataset, batch_sampler=sampler)\n",
    "\n",
    "val_loader = DataLoader(dataloader_test.dataset, batch_size=39, shuffle=True)\n",
    "test_loader = DataLoader(dataloader_test0.dataset, batch_size=39, shuffle=True)\n",
    "\n",
    "best_val_acc = 0\n",
    "\n",
    "# save_path = \"/content/drive/MyDrive/modelTA/checkpoint46out.pth\"\n",
    "\n",
    "# if os.path.exists(save_path):\n",
    "#     checkpoint = torch.load(save_path)\n",
    "#     model.load_state_dict(checkpoint['model_state_dict'])\n",
    "#     dg.load_state_dict(checkpoint['dg_state_dict'])\n",
    "#     optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "\n",
    "# Training loop\n",
    "try:\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        # model1.train()\n",
    "        dg.train()\n",
    "\n",
    "        correct, total, running_loss, total_dist_loss, total_mmd_loss, total_dom_loss = 0, 0, 0, 0, 0, 0\n",
    "        for batch in train_loader:\n",
    "            x_batch, yd_batch, mask_batch = batch\n",
    "            # if epoch >= 100:\n",
    "            #     noise = torch.randn_like(x_batch) * 0.2  # Adjust std as needed\n",
    "            #     x_batch = x_batch + noise\n",
    "            output = model(x_batch, mask_batch, channel_xy_tensor)\n",
    "            # output = model1(output)\n",
    "            # output = pool(output)\n",
    "\n",
    "            # Flatten\n",
    "            # output = output.view(output.shape[0], -1)\n",
    "            output = output.unsqueeze(1)\n",
    "            logits, weight, Feat_s, weighted_feature = dg(output)\n",
    "\n",
    "            # loss\n",
    "            feat_all = torch.stack(Feat_s, dim=0)  # [n_domains, batch_size, feat_dim]\n",
    "            domain_indices = yd_batch[:, 4]        # [batch_size]\n",
    "            unique_domains = domain_indices.unique(sorted=True)\n",
    "            labels = yd_batch[:, 0]\n",
    "\n",
    "            domain_id_list = domain_indices.unique(sorted=True)  # e.g., [1, 8, 101]\n",
    "            domain_id_to_index = {dom.item(): idx for idx, dom in enumerate(domain_id_list)}\n",
    "\n",
    "            grouped_feats = []\n",
    "            grouped_labels = []\n",
    "\n",
    "            for dom_id in domain_id_list:\n",
    "                domain_mask = domain_indices == dom_id  # Boolean mask\n",
    "                feats = feat_all[domain_id_to_index[dom_id.item()]][domain_mask]\n",
    "                labs = labels[domain_mask]\n",
    "\n",
    "                grouped_feats.append(feats)\n",
    "                grouped_labels.append(labs)\n",
    "\n",
    "            predictions = softmax(logits)\n",
    "            # Compute loss\n",
    "            dist_loss = dl(grouped_feats, grouped_labels, 0.1)\n",
    "            mmd_loss = mmdl(feat_all)\n",
    "            loss = criterion(logits, yd_batch[:, 0])\n",
    "            dom_loss = criterion_dom(weight, yd_batch[:, 4])\n",
    "            total_loss = loss + dist_weight * dist_loss + mmd_weigth * mmd_loss + dom_weight * dom_loss\n",
    "            running_loss += total_loss.item()\n",
    "            total_dist_loss += dist_loss.item()\n",
    "            total_mmd_loss += mmd_loss.item()\n",
    "            total_dom_loss += dom_loss.item()\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "            # Training accuracy\n",
    "            predicted_labels = torch.argmax(predictions, dim=1)\n",
    "            correct += (predicted_labels == yd_batch[:, 0]).sum().item()\n",
    "            total += yd_batch.size(0)\n",
    "\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_acc = correct / total * 100\n",
    "        # scheduler.step()\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        # model1.eval()\n",
    "        dg.eval()\n",
    "        val_loss, val_correct, val_total = 0, 0, 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                x_batch, yd_batch, mask_batch = batch\n",
    "                output = model(x_batch, mask_batch, channel_xy_tensor)\n",
    "                # output = model1(output)\n",
    "                # output = pool(output)\n",
    "\n",
    "                # Flatten\n",
    "                output = output.unsqueeze(1)\n",
    "                # print(output.shape)\n",
    "                logits, _ = dg(output)\n",
    "                predictions = softmax(logits)\n",
    "\n",
    "                # Compute loss\n",
    "                loss = criterion(logits, yd_batch[:, 0])\n",
    "                # dist_loss = dl(grouped_feats, grouped_labels, 0.1)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                # Validation accuracy\n",
    "                predicted_labels = torch.argmax(predictions, dim=1)\n",
    "                val_correct += (predicted_labels == yd_batch[:, 0]).sum().item()\n",
    "                val_total += yd_batch.size(0)\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc = val_correct / val_total * 100\n",
    "\n",
    "        test_loss, test_correct, test_total = 0, 0, 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                x_batch, yd_batch, mask_batch = batch\n",
    "                output = model(x_batch, mask_batch, channel_xy_tensor)\n",
    "                # output = model1(output)\n",
    "                # output = pool(output)\n",
    "\n",
    "                # Flatten\n",
    "                output = output.unsqueeze(1)\n",
    "                logits, _ = dg(output)\n",
    "                predictions = softmax(logits)\n",
    "\n",
    "                # Compute loss\n",
    "                loss = criterion(logits, yd_batch[:, 0])\n",
    "                # dist_loss = dl(grouped_feats, grouped_labels, 0.1)\n",
    "                test_loss += loss.item()\n",
    "\n",
    "                # Validation accuracy\n",
    "                predicted_labels = torch.argmax(predictions, dim=1)\n",
    "                test_correct += (predicted_labels == yd_batch[:, 0]).sum().item()\n",
    "                test_total += yd_batch.size(0)\n",
    "\n",
    "        test_loss /= len(test_loader)\n",
    "        test_acc = test_correct / test_total * 100\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            save_path = \"./checkpoint-TEST-BEST.pth\"\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'dg_state_dict': dg.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, save_path)\n",
    "\n",
    "            print(\"✅ Model saved.\")\n",
    "\n",
    "        save_path = \"./checkpoint-TEST.pth\"\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'dg_state_dict': dg.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "        }, save_path)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Dist Loss: {total_dist_loss/len(train_loader):.4f}, MMD Loss: {total_mmd_loss/len(train_loader):.4f}, Dom Loss {total_dom_loss/len(train_loader):.4f}, Train Acc: {train_acc:.2f}% | \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}% | \"\n",
    "              f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%\")\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n⛔ Training interrupted. Saving model...\")\n",
    "\n",
    "    save_path = \"./checkpoint-TEST.pth\"\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'dg_state_dict': dg.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }, save_path)\n",
    "\n",
    "    print(\"✅ Model saved. Exiting cleanly.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e1c4a4",
   "metadata": {},
   "source": [
    "PERFORMANCE EVALUATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7498875b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics by Domain:\n",
      "\n",
      "- Domain 13:\n",
      "    - Accuracy: 0.5612\n",
      "    - Class 0: F1 Score = 0.7671\n",
      "    - Class 1: F1 Score = 0.5385\n",
      "    - Class 2: F1 Score = 0.2667\n",
      "    - Misclassification Breakdown (rows = true class):\n",
      "        Class 0 misclassified as: 11.8% → class 1, 5.9% → class 2\n",
      "        Class 1 misclassified as: 17.1% → class 0, 22.9% → class 2\n",
      "        Class 2 misclassified as: 17.2% → class 0, 62.1% → class 1\n",
      "\n",
      "- Domain 14:\n",
      "    - Accuracy: 0.5882\n",
      "    - Class 0: F1 Score = 0.6301\n",
      "    - Class 1: F1 Score = 0.5556\n",
      "    - Class 2: F1 Score = 0.5581\n",
      "    - Misclassification Breakdown (rows = true class):\n",
      "        Class 0 misclassified as: 19.4% → class 1, 6.5% → class 2\n",
      "        Class 1 misclassified as: 34.5% → class 0, 13.8% → class 2\n",
      "        Class 2 misclassified as: 36.0% → class 0, 16.0% → class 1\n",
      "\n",
      "- Domain 15:\n",
      "    - Accuracy: 0.4894\n",
      "    - Class 0: F1 Score = 0.7273\n",
      "    - Class 1: F1 Score = 0.4615\n",
      "    - Class 2: F1 Score = 0.1818\n",
      "    - Misclassification Breakdown (rows = true class):\n",
      "        Class 0 misclassified as: 22.2% → class 1, 11.1% → class 2\n",
      "        Class 1 misclassified as: 14.8% → class 0, 18.5% → class 2\n",
      "        Class 2 misclassified as: 6.5% → class 0, 80.6% → class 1\n",
      "\n",
      "- Domain 16:\n",
      "    - Accuracy: 0.6869\n",
      "    - Class 0: F1 Score = 0.8493\n",
      "    - Class 1: F1 Score = 0.5902\n",
      "    - Class 2: F1 Score = 0.5938\n",
      "    - Misclassification Breakdown (rows = true class):\n",
      "        Class 0 misclassified as: 0.0% → class 1, 0.0% → class 2\n",
      "        Class 1 misclassified as: 14.3% → class 0, 34.3% → class 2\n",
      "        Class 2 misclassified as: 18.2% → class 0, 24.2% → class 1\n",
      "\n",
      "- Domain 17:\n",
      "    - Accuracy: 0.6957\n",
      "    - Class 0: F1 Score = 0.8197\n",
      "    - Class 1: F1 Score = 0.6038\n",
      "    - Class 2: F1 Score = 0.6571\n",
      "    - Misclassification Breakdown (rows = true class):\n",
      "        Class 0 misclassified as: 0.0% → class 1, 10.7% → class 2\n",
      "        Class 1 misclassified as: 12.9% → class 0, 35.5% → class 2\n",
      "        Class 2 misclassified as: 12.1% → class 0, 18.2% → class 1\n",
      "\n",
      "- Domain 18:\n",
      "    - Accuracy: 0.6386\n",
      "    - Class 0: F1 Score = 0.4898\n",
      "    - Class 1: F1 Score = 0.7119\n",
      "    - Class 2: F1 Score = 0.6897\n",
      "    - Misclassification Breakdown (rows = true class):\n",
      "        Class 0 misclassified as: 33.3% → class 1, 9.5% → class 2\n",
      "        Class 1 misclassified as: 17.2% → class 0, 10.3% → class 2\n",
      "        Class 2 misclassified as: 33.3% → class 0, 6.1% → class 1\n",
      "\n",
      "- Domain 19:\n",
      "    - Accuracy: 0.7816\n",
      "    - Class 0: F1 Score = 0.7419\n",
      "    - Class 1: F1 Score = 0.7857\n",
      "    - Class 2: F1 Score = 0.8214\n",
      "    - Misclassification Breakdown (rows = true class):\n",
      "        Class 0 misclassified as: 7.7% → class 1, 3.8% → class 2\n",
      "        Class 1 misclassified as: 23.3% → class 0, 3.3% → class 2\n",
      "        Class 2 misclassified as: 19.4% → class 0, 6.5% → class 1\n",
      "\n",
      "- Domain 20:\n",
      "    - Accuracy: 0.9239\n",
      "    - Class 0: F1 Score = 0.8966\n",
      "    - Class 1: F1 Score = 0.9310\n",
      "    - Class 2: F1 Score = 0.9412\n",
      "    - Misclassification Breakdown (rows = true class):\n",
      "        Class 0 misclassified as: 3.6% → class 1, 3.6% → class 2\n",
      "        Class 1 misclassified as: 6.7% → class 0, 3.3% → class 2\n",
      "        Class 2 misclassified as: 5.9% → class 0, 0.0% → class 1\n",
      "\n",
      "- Domain 21:\n",
      "    - Accuracy: 0.8085\n",
      "    - Class 0: F1 Score = 0.9474\n",
      "    - Class 1: F1 Score = 0.8052\n",
      "    - Class 2: F1 Score = 0.6667\n",
      "    - Misclassification Breakdown (rows = true class):\n",
      "        Class 0 misclassified as: 0.0% → class 1, 3.6% → class 2\n",
      "        Class 1 misclassified as: 0.0% → class 0, 3.1% → class 2\n",
      "        Class 2 misclassified as: 5.9% → class 0, 41.2% → class 1\n",
      "\n",
      "- Domain 22:\n",
      "    - Accuracy: 0.4725\n",
      "    - Class 0: F1 Score = 0.5714\n",
      "    - Class 1: F1 Score = 0.3256\n",
      "    - Class 2: F1 Score = 0.4737\n",
      "    - Misclassification Breakdown (rows = true class):\n",
      "        Class 0 misclassified as: 10.8% → class 1, 40.5% → class 2\n",
      "        Class 1 misclassified as: 14.8% → class 0, 59.3% → class 2\n",
      "        Class 2 misclassified as: 14.8% → class 0, 18.5% → class 1\n",
      "\n",
      "Overall Metrics:\n",
      "    - Overall Accuracy: 0.6645\n",
      "    - F1 Scores per Class:\n",
      "        Class 0: F1 Score = 0.7465\n",
      "        Class 1: F1 Score = 0.6353\n",
      "        Class 2: F1 Score = 0.6055\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "data = torch.load('./Dataset/eeg_data_weibo.pt', map_location=device)\n",
    "\n",
    "X_tensor = data['X']\n",
    "YD_tensor = data['YD']\n",
    "padding_masks_tensor = data['mask']\n",
    "channel_xy_tensor = data['channel_xy']\n",
    "\n",
    "X_tensor_half = data['X_val']\n",
    "YD_tensor_half = data['YD_val']\n",
    "padding_masks_tensor_half = data['mask_val']\n",
    "\n",
    "X_tensor_test = data['X_test']\n",
    "YD_tensor_test = data['YD_test']\n",
    "padding_masks_tensor_test = data['mask_test']\n",
    "\n",
    "dataset = MultisourceDataset(X_tensor, YD_tensor, padding_masks_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=10, shuffle=True)\n",
    "dataset_test = MultisourceDataset(X_tensor_half, YD_tensor_half, padding_masks_tensor_half)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=10, shuffle=True)\n",
    "dataset_test0 = MultisourceDataset(X_tensor_test, YD_tensor_test, padding_masks_tensor_test)\n",
    "dataloader_test0 = DataLoader(dataset_test0, batch_size=10, shuffle=True)\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "\n",
    "# === Initialize ===\n",
    "seed = 42\n",
    "num_channels = 46\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load DataLoader\n",
    "test_loader = DataLoader(dataloader_test0.dataset, batch_size=39, shuffle=False)\n",
    "\n",
    "# Load models\n",
    "model = SpatialAttention(num_channels, 48, dropout=False, seed=seed).to(device)\n",
    "dg = DG_Network(3, 23, channels=num_channels, feature_size=3328).to(device)\n",
    "softmax = nn.Softmax(dim=1)\n",
    "\n",
    "# Load checkpoint\n",
    "save_path = \"./ProposedModel/checkpoint46out.pth\"\n",
    "if os.path.exists(save_path):\n",
    "    checkpoint = torch.load(save_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    dg.load_state_dict(checkpoint['dg_state_dict'])\n",
    "\n",
    "model.eval()\n",
    "dg.eval()\n",
    "\n",
    "# === Storage for metrics ===\n",
    "domain_predictions = defaultdict(list)\n",
    "domain_targets = defaultdict(list)\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "\n",
    "# === Inference ===\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        x_batch, yd_batch, mask_batch = batch\n",
    "        x_batch, mask_batch = x_batch.to(device), mask_batch.to(device)\n",
    "        output = model(x_batch, mask_batch, channel_xy_tensor.to(device))\n",
    "\n",
    "        output = output.unsqueeze(1)  # (B, 1, C_out, T)\n",
    "        logits, _ = dg(output)\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "\n",
    "        domains = yd_batch[:, 4].cpu().numpy()\n",
    "        targets = yd_batch[:, 0].cpu().numpy()\n",
    "        preds = predictions.cpu().numpy()\n",
    "\n",
    "        # Save for overall metrics\n",
    "        all_preds.extend(preds)\n",
    "        all_targets.extend(targets)\n",
    "\n",
    "        # Save for per-domain metrics\n",
    "        for domain, target, pred in zip(domains, targets, preds):\n",
    "            domain_predictions[domain].append(pred)\n",
    "            domain_targets[domain].append(target)\n",
    "\n",
    "# === Print per-domain metrics ===\n",
    "print(\"Metrics by Domain:\")\n",
    "for domain in sorted(domain_predictions.keys()):\n",
    "    print(f\"\\n- Domain {domain}:\")\n",
    "\n",
    "    y_true = domain_targets[domain]\n",
    "    y_pred = domain_predictions[domain]\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    print(f\"    - Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    classes = sorted(set(y_true))\n",
    "    f1_scores = f1_score(y_true, y_pred, average=None, labels=classes)\n",
    "    for cls, f1 in zip(classes, f1_scores):\n",
    "        print(f\"    - Class {cls}: F1 Score = {f1:.4f}\")\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=classes)\n",
    "    cm_percent = cm.astype(float) / cm.sum(axis=1, keepdims=True)\n",
    "\n",
    "    print(f\"    - Misclassification Breakdown (rows = true class):\")\n",
    "    for i, cls in enumerate(classes):\n",
    "        breakdown = []\n",
    "        for j, pred_cls in enumerate(classes):\n",
    "            pct = cm_percent[i, j] * 100\n",
    "            if i == j:\n",
    "                continue\n",
    "            breakdown.append(f\"{pct:.1f}% → class {pred_cls}\")\n",
    "        if breakdown:\n",
    "            print(f\"        Class {cls} misclassified as: {', '.join(breakdown)}\")\n",
    "        else:\n",
    "            print(f\"        Class {cls} has no misclassifications.\")\n",
    "\n",
    "# === Overall Metrics ===\n",
    "overall_accuracy = accuracy_score(all_targets, all_preds)\n",
    "classes = sorted(set(all_targets))\n",
    "f1_per_class = f1_score(all_targets, all_preds, average=None, labels=classes)\n",
    "\n",
    "print(\"\\nOverall Metrics:\")\n",
    "print(f\"    - Overall Accuracy: {overall_accuracy:.4f}\")\n",
    "print(f\"    - F1 Scores per Class:\")\n",
    "for cls, f1 in zip(classes, f1_per_class):\n",
    "    print(f\"        Class {cls}: F1 Score = {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5db0a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics by Domain:\n",
      "\n",
      "- Domain 13:\n",
      "    - Accuracy: 0.5918\n",
      "    - Class 0: F1 Score = 0.8358\n",
      "    - Class 1: F1 Score = 0.4688\n",
      "    - Class 2: F1 Score = 0.4615\n",
      "    - Misclassification Breakdown (rows = true class):\n",
      "        Class 0 misclassified as: 8.8% → class 1, 8.8% → class 2\n",
      "        Class 1 misclassified as: 5.7% → class 0, 51.4% → class 2\n",
      "        Class 2 misclassified as: 10.3% → class 0, 37.9% → class 1\n",
      "\n",
      "- Domain 14:\n",
      "    - Accuracy: 0.4000\n",
      "    - Class 0: F1 Score = 0.4615\n",
      "    - Class 1: F1 Score = 0.1333\n",
      "    - Class 2: F1 Score = 0.5333\n",
      "    - Misclassification Breakdown (rows = true class):\n",
      "        Class 0 misclassified as: 25.8% → class 1, 25.8% → class 2\n",
      "        Class 1 misclassified as: 51.7% → class 0, 37.9% → class 2\n",
      "        Class 2 misclassified as: 16.0% → class 0, 20.0% → class 1\n",
      "\n",
      "- Domain 15:\n",
      "    - Accuracy: 0.3511\n",
      "    - Class 0: F1 Score = 0.2857\n",
      "    - Class 1: F1 Score = 0.3864\n",
      "    - Class 2: F1 Score = 0.3529\n",
      "    - Misclassification Breakdown (rows = true class):\n",
      "        Class 0 misclassified as: 66.7% → class 1, 13.9% → class 2\n",
      "        Class 1 misclassified as: 14.8% → class 0, 22.2% → class 2\n",
      "        Class 2 misclassified as: 6.5% → class 0, 64.5% → class 1\n",
      "\n",
      "- Domain 16:\n",
      "    - Accuracy: 0.3838\n",
      "    - Class 0: F1 Score = 0.3729\n",
      "    - Class 1: F1 Score = 0.3284\n",
      "    - Class 2: F1 Score = 0.4444\n",
      "    - Misclassification Breakdown (rows = true class):\n",
      "        Class 0 misclassified as: 38.7% → class 1, 25.8% → class 2\n",
      "        Class 1 misclassified as: 25.7% → class 0, 42.9% → class 2\n",
      "        Class 2 misclassified as: 24.2% → class 0, 27.3% → class 1\n",
      "\n",
      "- Domain 17:\n",
      "    - Accuracy: 0.5109\n",
      "    - Class 0: F1 Score = 0.6667\n",
      "    - Class 1: F1 Score = 0.4545\n",
      "    - Class 2: F1 Score = 0.4375\n",
      "    - Misclassification Breakdown (rows = true class):\n",
      "        Class 0 misclassified as: 21.4% → class 1, 14.3% → class 2\n",
      "        Class 1 misclassified as: 9.7% → class 0, 41.9% → class 2\n",
      "        Class 2 misclassified as: 15.2% → class 0, 42.4% → class 1\n",
      "\n",
      "- Domain 18:\n",
      "    - Accuracy: 0.5422\n",
      "    - Class 0: F1 Score = 0.5484\n",
      "    - Class 1: F1 Score = 0.1081\n",
      "    - Class 2: F1 Score = 0.7761\n",
      "    - Misclassification Breakdown (rows = true class):\n",
      "        Class 0 misclassified as: 14.3% → class 1, 4.8% → class 2\n",
      "        Class 1 misclassified as: 69.0% → class 0, 24.1% → class 2\n",
      "        Class 2 misclassified as: 12.1% → class 0, 9.1% → class 1\n",
      "\n",
      "- Domain 19:\n",
      "    - Accuracy: 0.7931\n",
      "    - Class 0: F1 Score = 0.7368\n",
      "    - Class 1: F1 Score = 0.7931\n",
      "    - Class 2: F1 Score = 0.8475\n",
      "    - Misclassification Breakdown (rows = true class):\n",
      "        Class 0 misclassified as: 15.4% → class 1, 3.8% → class 2\n",
      "        Class 1 misclassified as: 16.7% → class 0, 6.7% → class 2\n",
      "        Class 2 misclassified as: 16.1% → class 0, 3.2% → class 1\n",
      "\n",
      "- Domain 20:\n",
      "    - Accuracy: 0.6630\n",
      "    - Class 0: F1 Score = 0.5938\n",
      "    - Class 1: F1 Score = 0.4815\n",
      "    - Class 2: F1 Score = 0.8788\n",
      "    - Misclassification Breakdown (rows = true class):\n",
      "        Class 0 misclassified as: 28.6% → class 1, 3.6% → class 2\n",
      "        Class 1 misclassified as: 50.0% → class 0, 6.7% → class 2\n",
      "        Class 2 misclassified as: 5.9% → class 0, 8.8% → class 1\n",
      "\n",
      "- Domain 21:\n",
      "    - Accuracy: 0.7766\n",
      "    - Class 0: F1 Score = 0.9310\n",
      "    - Class 1: F1 Score = 0.5957\n",
      "    - Class 2: F1 Score = 0.7711\n",
      "    - Misclassification Breakdown (rows = true class):\n",
      "        Class 0 misclassified as: 0.0% → class 1, 3.6% → class 2\n",
      "        Class 1 misclassified as: 6.2% → class 0, 50.0% → class 2\n",
      "        Class 2 misclassified as: 2.9% → class 0, 2.9% → class 1\n",
      "\n",
      "- Domain 22:\n",
      "    - Accuracy: 0.3407\n",
      "    - Class 0: F1 Score = 0.2000\n",
      "    - Class 1: F1 Score = 0.2979\n",
      "    - Class 2: F1 Score = 0.4471\n",
      "    - Misclassification Breakdown (rows = true class):\n",
      "        Class 0 misclassified as: 24.3% → class 1, 62.2% → class 2\n",
      "        Class 1 misclassified as: 14.8% → class 0, 59.3% → class 2\n",
      "        Class 2 misclassified as: 14.8% → class 0, 14.8% → class 1\n",
      "\n",
      "Overall Metrics:\n",
      "    - Overall Accuracy: 0.5344\n",
      "    - F1 Scores per Class:\n",
      "        Class 0: F1 Score = 0.5744\n",
      "        Class 1: F1 Score = 0.4188\n",
      "        Class 2: F1 Score = 0.5982\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "data = torch.load('./Dataset/eeg_data_weibo.pt', map_location=device)\n",
    "\n",
    "X_tensor = data['X']\n",
    "YD_tensor = data['YD']\n",
    "padding_masks_tensor = data['mask']\n",
    "channel_xy_tensor = data['channel_xy']\n",
    "\n",
    "X_tensor_half = data['X_val']\n",
    "YD_tensor_half = data['YD_val']\n",
    "padding_masks_tensor_half = data['mask_val']\n",
    "\n",
    "X_tensor_test = data['X_test']\n",
    "YD_tensor_test = data['YD_test']\n",
    "padding_masks_tensor_test = data['mask_test']\n",
    "\n",
    "dataset = MultisourceDataset(X_tensor, YD_tensor, padding_masks_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=10, shuffle=True)\n",
    "dataset_test = MultisourceDataset(X_tensor_half, YD_tensor_half, padding_masks_tensor_half)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=10, shuffle=True)\n",
    "dataset_test0 = MultisourceDataset(X_tensor_test, YD_tensor_test, padding_masks_tensor_test)\n",
    "dataloader_test0 = DataLoader(dataset_test0, batch_size=10, shuffle=True)\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "\n",
    "# === Initialize ===\n",
    "seed = 42\n",
    "num_channels = 46\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load DataLoader\n",
    "test_loader = DataLoader(dataloader_test0.dataset, batch_size=39, shuffle=False)\n",
    "\n",
    "# Load models\n",
    "model = SpatialAttention(num_channels, 48, dropout=False, seed=seed).to(device)\n",
    "dg = DG_Network(3, 10, channels=num_channels, feature_size=3328).to(device)\n",
    "softmax = nn.Softmax(dim=1)\n",
    "\n",
    "# Load checkpoint\n",
    "save_path = \"./ProposedModel/checkpoint46outWeibo.pth\"\n",
    "if os.path.exists(save_path):\n",
    "    checkpoint = torch.load(save_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    dg.load_state_dict(checkpoint['dg_state_dict'])\n",
    "\n",
    "model.eval()\n",
    "dg.eval()\n",
    "\n",
    "# === Storage for metrics ===\n",
    "domain_predictions = defaultdict(list)\n",
    "domain_targets = defaultdict(list)\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "\n",
    "# === Inference ===\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        x_batch, yd_batch, mask_batch = batch\n",
    "        x_batch, mask_batch = x_batch.to(device), mask_batch.to(device)\n",
    "        output = model(x_batch, mask_batch, channel_xy_tensor.to(device))\n",
    "\n",
    "        output = output.unsqueeze(1)  # (B, 1, C_out, T)\n",
    "        logits, _ = dg(output)\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "\n",
    "        domains = yd_batch[:, 4].cpu().numpy()\n",
    "        targets = yd_batch[:, 0].cpu().numpy()\n",
    "        preds = predictions.cpu().numpy()\n",
    "\n",
    "        # Save for overall metrics\n",
    "        all_preds.extend(preds)\n",
    "        all_targets.extend(targets)\n",
    "\n",
    "        # Save for per-domain metrics\n",
    "        for domain, target, pred in zip(domains, targets, preds):\n",
    "            domain_predictions[domain].append(pred)\n",
    "            domain_targets[domain].append(target)\n",
    "\n",
    "# === Print per-domain metrics ===\n",
    "print(\"Metrics by Domain:\")\n",
    "for domain in sorted(domain_predictions.keys()):\n",
    "    print(f\"\\n- Domain {domain}:\")\n",
    "\n",
    "    y_true = domain_targets[domain]\n",
    "    y_pred = domain_predictions[domain]\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    print(f\"    - Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    classes = sorted(set(y_true))\n",
    "    f1_scores = f1_score(y_true, y_pred, average=None, labels=classes)\n",
    "    for cls, f1 in zip(classes, f1_scores):\n",
    "        print(f\"    - Class {cls}: F1 Score = {f1:.4f}\")\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=classes)\n",
    "    cm_percent = cm.astype(float) / cm.sum(axis=1, keepdims=True)\n",
    "\n",
    "    print(f\"    - Misclassification Breakdown (rows = true class):\")\n",
    "    for i, cls in enumerate(classes):\n",
    "        breakdown = []\n",
    "        for j, pred_cls in enumerate(classes):\n",
    "            pct = cm_percent[i, j] * 100\n",
    "            if i == j:\n",
    "                continue\n",
    "            breakdown.append(f\"{pct:.1f}% → class {pred_cls}\")\n",
    "        if breakdown:\n",
    "            print(f\"        Class {cls} misclassified as: {', '.join(breakdown)}\")\n",
    "        else:\n",
    "            print(f\"        Class {cls} has no misclassifications.\")\n",
    "\n",
    "# === Overall Metrics ===\n",
    "overall_accuracy = accuracy_score(all_targets, all_preds)\n",
    "classes = sorted(set(all_targets))\n",
    "f1_per_class = f1_score(all_targets, all_preds, average=None, labels=classes)\n",
    "\n",
    "print(\"\\nOverall Metrics:\")\n",
    "print(f\"    - Overall Accuracy: {overall_accuracy:.4f}\")\n",
    "print(f\"    - F1 Scores per Class:\")\n",
    "for cls, f1 in zip(classes, f1_per_class):\n",
    "    print(f\"        Class {cls}: F1 Score = {f1:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
